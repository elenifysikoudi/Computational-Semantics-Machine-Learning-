{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A6: Natural Language Inference using Neural Networks\n",
    "\n",
    "by Adam Ek, Bill Noble, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "\n",
    "In this lab we will work with neural networks for natural language inference. Our task is: given a premise sentence P and hypothesis H, what entailment relationship holds between them? Is H entailed by P, contradicted by P or neutral towards P?\n",
    "\n",
    "Given a sentence P, if H definitely describe something true given P then it is an **entailment**. If H describe something that's *maybe* true given P, it's **neutral**, and if H describe something that's definitely *false* given P it's a **contradiction**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore natural language inference using neural networks on the SNLI dataset, described in [1]. The dataset can be downloaded [here](https://nlp.stanford.edu/projects/snli/), but unfortunately that dataset is down at the moment. Instead, we will use the version uploaded to [HuggingFace](https://huggingface.co/datasets/stanfordnlp/snli) available through the `datasets` library. See the [documentation](https://huggingface.co/docs/datasets/v2.19.0/loading#hugging-face-hub) for loading a dataset from the HuggingFace hub.\n",
    "\n",
    "The (simplified) data is organized as follows:\n",
    "\n",
    "* Column 1: Premise\n",
    "* Column 2: Hypothesis\n",
    "* Column 3: Relation\n",
    "\n",
    "Like in the previous lab, we'll need to build a dataloader. You can adapt your code from the previous lab to the new dataset. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(description='', citation='', homepage='', license='', features={'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='snli', config_name='plain_text', version=0.0.0, splits={'test': SplitInfo(name='test', num_bytes=1260154, num_examples=10000, shard_lengths=None, dataset_name='snli'), 'validation': SplitInfo(name='validation', num_bytes=1264286, num_examples=10000, shard_lengths=None, dataset_name='snli'), 'train': SplitInfo(name='train', num_bytes=65953155, num_examples=550152, shard_lengths=None, dataset_name='snli')}, download_checksums={'hf://datasets/stanfordnlp/snli@cdb5c3d5eed6ead6e5a341c8e56e669bb666725b/plain_text/test-00000-of-00001.parquet': {'num_bytes': 411531, 'checksum': None}, 'hf://datasets/stanfordnlp/snli@cdb5c3d5eed6ead6e5a341c8e56e669bb666725b/plain_text/validation-00000-of-00001.parquet': {'num_bytes': 413157, 'checksum': None}, 'hf://datasets/stanfordnlp/snli@cdb5c3d5eed6ead6e5a341c8e56e669bb666725b/plain_text/train-00000-of-00001.parquet': {'num_bytes': 19614612, 'checksum': None}}, download_size=20439300, post_processing_size=None, dataset_size=68477595, size_in_bytes=88916895)\n"
     ]
    }
   ],
   "source": [
    "#   LOADING OUR DATA - SNLI DATASET &\n",
    "#   DOING OUR IMPORTS:\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset('stanfordnlp/snli')\n",
    "ds_builder = load_dataset_builder('stanfordnlp/snli')\n",
    "ex = dataset['train'][0]\n",
    "#print(ex)\n",
    "print(ds_builder.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f5b8aa1a084c32bf9cda30d968b92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8a06755c574d71bc2780ffb8f9863d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bce207f6a7e443896d70f2cacc10809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/549367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATING OUR DATASET FOR THE DATALOADER (will be easier since we are using a built-in dataset from HuggingFace):\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')   \n",
    "dataset = dataset.filter(lambda example: example[\"label\"] != -1)\n",
    "dataset.set_format(type='torch', columns=['premise', 'hypothesis', 'label'])\n",
    "# our datasets:\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "\n",
    "def collate_function(batch):\n",
    "    \"\"\"Merges a list of samples to form a mini-batch.\n",
    "    This function will be used as an argument for the DataLoader below.\"\"\"\n",
    "    batch_premise = [example['premise'] for example in batch]   # a list of batch size premises.\n",
    "    batch_hypothesis = [example['hypothesis'] for example in batch] # a list of batch size hypotheses.\n",
    "    batch_label = [example['label'] for example in batch] # a list of batch size labels (as tensors).\n",
    "\n",
    "    return {'premise': batch_premise, 'hypothesis': batch_hypothesis, 'label': batch_label}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dataset comes as a dictionary-like object with three splits: `'test'`, `'train'`, and `'validation'`. Each item is a dictionary containing a `'premise'`, `'hypothesis'`, and `'label'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30152de77c64e20963bc52b2a5ec5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/549367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2690f0b8a6fb4c49a30e1766ffcfdd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49402f1c034241818ee1f9774106473f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# smaller datasets for debugging purposes\n",
    "train_dataset = train_dataset.filter(lambda e, i: i<1000, with_indices=True)\n",
    "validation_dataset = validation_dataset.filter(lambda e, i: i<1000, with_indices=True)\n",
    "test_dataset = test_dataset.filter(lambda e, i: i<1000, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': ['A boy is jumping on skateboard in the middle of a red bridge.', 'A woman and a child holding on to the railing while on trolley.', 'A white and brown dog is leaping through the air.', 'A man in a gold skirt sits in front of the computer.', 'A car sinking in water.', 'Two dogs biting another dog in a field.', 'A young woman packs belongings into a black suitcase.', 'Two young girls are playing outside in a non-urban environment.', 'A woman wearing a green and pink dress is dancing with someone wearing a blue top with white pants.', 'There is a woman holding a baby, along with a man with a save the children bag.', \"A model posing to look as if she's a real female soccer player.\", 'Two people dancing, wearing dance costumes.', 'Four guys in wheelchairs on a basketball court two are trying to grab a basketball in midair.', 'A man holds a clipboard and a pen as a woman looks at them.', \"Woman in white in foreground and a man slightly behind walking with a sign for John's Pizza and Gyro in the background.\", 'A lady is on the floor packing a suitcase.', 'A crowded city during daytime.', 'three bikers stop in town.', 'many people relax in the yard.', 'Brown dog treads through water.', 'A meeting of young people sitting at a conference table.', 'A woman is walking across the street eating a banana, while a man is following with his briefcase.', 'A woman in a blue shirt and green hat looks up at the camera.', 'A boy is drinking out of a water fountain shaped like a woman.', 'People on bicycles waiting at an intersection.', 'Two adults, one female in white, with shades and one male, gray clothes, walking across a street, away from a eatery with a blurred image of a dark colored red shirted person in the foreground.', 'A woman talks to two other women and a man with notepads in an office building with large windows.', 'A foreign family is walking along a dirt path next to the water.', 'A dog drops a red disc on a beach.', 'A little boy underwater in a pool, holding a plastic dinosaur.', 'A white bike is tied to a street sign.', 'A mountain biker jumping a slope outdoors in a forest area.'], 'hypothesis': ['The boy skates down the sidewalk.', 'The people are on a trolley in san Francisco.', 'The dog is taking a nap.', 'A man in blue pants sitting and watching the television.', 'A truck drives down a country road in the sunshine.', 'dogs bitting', 'A young woman distributes belongings into a black box.', 'Two girls are playing outside.', 'The dancing woman is on the grass.', 'A group of people are possing for an add.', 'A woman is trying to model for a sports magazine.', 'Two people are eating dinner.', 'Four guys are playing basketball.', \"A woman is looking at a man's possessions\", 'The woman and man are outdoors.', 'She is packing.', 'A city filled with people in the middle of the daytime.', 'Five bikers are riding on the road.', 'A group of people are outside.', 'Brown dog is sleeping next to the water', 'young people are learning about politics', 'The woman is eating a banana.', 'A woman wearing a blue shirt and green hat smiles at the camera', 'A sculptor takes a drink from a fountain that he made that looks like his girlfriend.', 'The people are on skateboards.', 'The people are related.', 'a woman in an office building talks to a group of three taking notes on a notepad', 'A group of locals run on a dirt trail next to a frozen stream.', 'a dog catch the ball on a beach', 'The toy is waterproof.', 'the car is parked at the sign', 'The biker is using a small bike to jump a hole.'], 'label': [tensor(2), tensor(1), tensor(2), tensor(2), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2), tensor(0), tensor(2), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(2), tensor(2), tensor(1), tensor(2), tensor(1)]}\n"
     ]
    }
   ],
   "source": [
    "# DATALOADERS FOR OUR TRAIN, VALIDATION AND TEST DATA:\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_function)\n",
    "dataloader_validation = DataLoader(validation_dataset, batch_size=32, shuffle=True, collate_fn=collate_function)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_function)\n",
    "\n",
    "for batch in dataloader_train:\n",
    "    print(batch)\n",
    "    break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "This data does not come pre-tokenized. Instead of training our own tokenizer, we can use the BERT tokenizer like in the preivous assignment. Even though we aren't using BERT the tokenizer works with any model. See the documentation on [using a pretrained tokenizer](https://huggingface.co/docs/tokenizers/en/quicktour#using-a-pretrained-tokenizer). **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_word_to_idx(data_loader):\n",
    "    \"\"\"Goes through a dataloader's batches, transforms tokens into indices, creates a tensor out of the \n",
    "    premise and hypothesis. Labels are already tensors so they simply get appended into tokenized_batch as they are.\"\"\"\n",
    "    tokenized_batch = []  # tokenized batches get appended here\n",
    "    for batch in data_loader:\n",
    "        tokenized_premises = tokenizer(batch['premise'],padding='longest',truncation=True,return_tensors=\"pt\")\n",
    "        tokenized_hypotheses = tokenizer(batch['hypothesis'],padding='longest',truncation=True,return_tensors=\"pt\")\n",
    "        # Appending tokenized premise, hypothesis, and label to the tokenized batch\n",
    "        tokenized_batch.append({\n",
    "            'premises_input_ids': tokenized_premises['input_ids'],\n",
    "            'premises_attention_mask': tokenized_premises['attention_mask'],\n",
    "            #'premises_token_type_ids': tokenized_premises['token_type_ids'],\n",
    "            'hypotheses_input_ids': tokenized_hypotheses['input_ids'],\n",
    "            'hypotheses_attention_mask': tokenized_hypotheses['attention_mask'],\n",
    "            #'hypotheses_token_type_ids': tokenized_hypotheses['token_type_ids'],\n",
    "            'label': batch['label']\n",
    "            })\n",
    "\n",
    "    #print(f\"This is the [0] from the tokenized batch: {tokenized_batch[0]}\")        \n",
    "    return tokenized_batch\n",
    "\n",
    "training_data = tokenize_word_to_idx(dataloader_train)\n",
    "validation_data = tokenize_word_to_idx(dataloader_validation)\n",
    "testing_data = tokenize_word_to_idx(dataloader_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premises_input_ids': tensor([[  101,  1037,  2158,  1999,  1037,  2630,  3797,  7719,  2648,  2894,\n",
      "          2007,  1037,  7433,  6277,  4201,  2041,  1999,  2392,  1997,  2032,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1037,  2450,  2007,  1037,  2304,  6598,  7365,  2627,  2019,\n",
      "          7254,  3185, 13082,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1037,  2158,  2006,  1037,  2395,  1999,  1037,  4408,  1056,\n",
      "          1011,  3797,  4324,  2070,  4066,  1997, 13855,  2875,  1037,  2450,\n",
      "          1999,  1037,  5061,  1056,  1011,  3797,  1998, 13178,  1012,   102],\n",
      "        [  101,  1037,  2158,  1998,  1037,  2450,  2024,  3061,  2279,  2000,\n",
      "         10801,  1010,  3331,  2096,  2178,  2158,  3504,  2012,  2060, 10801,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0]]), 'premises_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'hypotheses_input_ids': tensor([[  101,  2048,  9750,  2273,  2652,  8040,  2527, 11362,  1012,   102,\n",
      "             0,     0,     0],\n",
      "        [  101,  1037,  2450,  2003,  9344,  2039,  1012,   102,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  1996,  2158,  2192,  7840,  1996,  3526,  3042,  2000,  1996,\n",
      "          2450,   102,     0],\n",
      "        [  101,  2093,  2111,  2024,  2559,  2012,  4169,  2012,  1037,  2082,\n",
      "          4189,  1012,   102]]), 'hypotheses_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'label': [tensor(2), tensor(2), tensor(2), tensor(2)]}\n",
      "{'premises_input_ids': tensor([[  101,  2116,  2111,  9483,  1999,  1996,  4220,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1037,  2158,  4755,  1037,  7170,  1997,  4840,  3622,  8378,\n",
      "          2006,  2482,  2007,  7787,  1999,  1996,  2103,  4534,  1010,  2004,\n",
      "          1037,  2450,  7365,  2875,  2032,  1012,   102],\n",
      "        [  101,  1037,  2450,  2003,  2770,  1037,  8589,  1999,  1037,  2380,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2048,  2111,  2007, 21066,  1010,  2028,  1999,  2392,  2770,\n",
      "          2007,  1037,  7997,  1998,  2028,  1999,  2067,  5559,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0]]), 'premises_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'hypotheses_input_ids': tensor([[  101,  1037,  2155, 15646,  1996, 11559,  2154,  1999,  2037, 16125,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2619,  2003,  2000,  3436, 14555,  1999,  2019,  3923,  4292,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  2450,  2003, 19350,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2028,  2711,  2770,  2279,  2000,  2037,  7997,  2007,  1996,\n",
      "          2711,  5559,  2037,  7997,  2369,  2068,  1012,   102]]), 'hypotheses_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'label': [tensor(1), tensor(0), tensor(0), tensor(0)]}\n"
     ]
    }
   ],
   "source": [
    "for batch in training_data[:2]:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll build the model for predicting the relationship between H and P.\n",
    "\n",
    "We will process each sentence using an LSTM. Then, we will construct some representation of the sentence. When we have a representation for H and P, we will combine them into one vector which we can use to predict the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model described in [2], the BiLSTM with max-pooling model. The procedure for the model is roughly:\n",
    "\n",
    "    1) Encode the Hypothesis and the Premise using one shared bidirectional LSTM (or two different LSTMS)\n",
    "    2) Perform max over the tokens in the premise and the hypothesis\n",
    "    3) Combine the encoded premise and encoded hypothesis into one representation\n",
    "    4) Predict the relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a representation of a sentence\n",
    "\n",
    "Let's first consider step 2 where we perform pooling. There is a builtin function in pytorch for this, but we'll implement it from scratch. \n",
    "\n",
    "Let's consider the general case, what we want to do for these methods is apply some function $f$ along dimension $i$, and we want to do this for all $i$'s. As an example we consider the matrix S with size ``(N, D)`` where N is the number of words and D the number of dimensions:\n",
    "\n",
    "$S = \\begin{bmatrix}\n",
    "    s_{11} & s_{12} & s_{13} & \\dots  & s_{1d} \\\\\n",
    "    s_{21} & s_{22} & s_{23} & \\dots  & s_{2d} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    s_{n1} & s_{n2} & s_{n3} & \\dots  & s_{nd}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "What we want to do is apply our function $f$ on each dimension, taking the input $s_{1d}, s_{2d}, ..., s_{nd}$ and generating the output $x_d$. \n",
    "\n",
    "You will implement the max pooling method. When performing max-pooling, $max$ will be the function which selects a _maximum_ value from a vector and $x$ is the output, thus for each dimension $d$ in our output $x$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_d = max(s_{1d}, s_{2d}, ..., s_{nd})\n",
    "\\end{equation}\n",
    "\n",
    "This operation will reduce a batch of size ``(batch_size, num_words, dimensions)`` to ``(batch_size, dimensions)`` meaning that we now have created a sentence representation based on the content of the representation at each token position. \n",
    "\n",
    "Create a function that takes as input a tensor of size ``(batch_size, num_words, dimensions)`` then performs max pooling and returns the result (the output should be of size: ```(batch_size, dimensions)```). [**4 Marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def max_pooling(input_tensor):\n",
    "  output_tensor,_ = torch.max(input_tensor, dim=1)\n",
    "  return output_tensor\n",
    "\n",
    "test_unpooled = torch.rand(32, 100, 512)\n",
    "test_pooled = max_pooling(test_unpooled)\n",
    "#print(test_pooled.size()) # should be torch.Size([32, 512])\n",
    "#for batch in training_data[:5]:\n",
    "#  print(batch['premises_input_ids'])\n",
    "#  print(max_pooling(batch['premises_input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining sentence representations\n",
    "\n",
    "Next, we need to combine the premise and hypothesis into one representation. We will do this by concatenating four tensors (the final size of our tensor $X$ should be ``(batch_size, 4d)`` where ``d`` is the number of dimensions that you use): \n",
    "\n",
    "$$X = [P; H; |P-H|; P \\cdot H]$$\n",
    "\n",
    "Here, what we do is concatenating P, H, P times H, and the absolute value of P minus H, then return the result.\n",
    "\n",
    "Implement the function. **[4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2048])\n"
     ]
    }
   ],
   "source": [
    "def combine_premise_and_hypothesis(hypothesis, premise):\n",
    "    output = torch.cat((premise, hypothesis, torch.abs(premise - hypothesis), premise * hypothesis), dim=1)\n",
    "    return output\n",
    "\n",
    "test_hypothesis = test_pooled.clone()\n",
    "test_premise = test_pooled.clone()\n",
    "test_combined = combine_premise_and_hypothesis(test_hypothesis, test_premise)\n",
    "print(test_combined.size()) # should be torch.Size([32, 2048])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"comment\"> `max_pooling` and `combine_premise_and_hypothesis` will used as part of the model! They are expected to get an input *after* running the premise and hypothesis through the embedding and RNN. Doing this adds a new dimension -- the dimension of the sentence representation with the `hidden_size` of the model. When you run max_pooling on the raw input tokens it reduces the batch dimension instead since there is no hidden dimension.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "Finally, we can build the model according to the procedure given previously by using the functions we defined above. Additionaly, in the model you should use *dropout*. For efficiency purposes, it's acceptable to only train the model with either max or mean pooling. \n",
    "\n",
    "Implement the model [**8 marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, embedding_dim, labels_size,dropout_rate=0.5):\n",
    "        super(SNLIModel,self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.classifier = nn.Linear(in_features=hidden_size*4, out_features=labels_size)\n",
    "        \n",
    "    def forward(self, premise, hypothesis):\n",
    "        p = self.embeddings(premise)\n",
    "        h = self.embeddings(hypothesis)\n",
    "        \n",
    "        p_out,_= self.rnn(p)\n",
    "        h_out,_ = self.rnn(h)\n",
    "\n",
    "        p_pooled = max_pooling(p_out)\n",
    "        h_pooled = max_pooling(h_out)\n",
    "\n",
    "        p_pooled = self.dropout(p_pooled)\n",
    "        h_pooled = self.dropout(h_pooled)\n",
    "        \n",
    "        ph_representation = combine_premise_and_hypothesis(h_pooled,p_pooled)\n",
    "        ph_representation = self.dropout(ph_representation)\n",
    "        predictions = self.classifier(ph_representation)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SNLIModel(tokenizer.vocab_size,512,512,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, implement the training and testing of the model. SNLI can take a very long time to train, so I suggest you only run it for one or two epochs. **[10 marks]** \n",
    "\n",
    "**Tip for efficiency:** *when developing your model, try training and testing the model on one batch (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss : 0.04223360407377186\n",
      "Epoch 2, Loss : 0.04066039649023878\n",
      "Epoch 3, Loss : 0.033160697478706425\n",
      "Epoch 4, Loss : 0.013330697315950601\n",
      "Epoch 5, Loss : 0.002337848700278755\n",
      "Epoch 6, Loss : 0.0009390919954057608\n",
      "Epoch 7, Loss : 0.0004217031505504565\n",
      "Epoch 8, Loss : 0.0002617694609625687\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.54      0.65      0.59       344\n",
      "contradiction       0.55      0.41      0.47       327\n",
      "      neutral       0.50      0.52      0.51       329\n",
      "\n",
      "     accuracy                           0.53      1000\n",
      "    macro avg       0.53      0.53      0.52      1000\n",
      " weighted avg       0.53      0.53      0.52      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "epochs = 8\n",
    "\n",
    "#We were unable to access the server these last couple of days so we used only a small portion of the dataset and didn't get a chance to test the whole dataset.\n",
    "\n",
    "loss_function = CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(),lr=0.003)\n",
    "model = model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in training_data:\n",
    "        optimizer.zero_grad()\n",
    "        premises = batch['premises_input_ids']\n",
    "        hypothesis = batch['hypotheses_input_ids']\n",
    "        labels = torch.tensor(batch['label'])     \n",
    "        #print(labels)\n",
    "        outputs = model(premises,hypothesis)\n",
    "        #print(outputs)\n",
    "        loss = loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss : {total_loss/len(training_data)}')    \n",
    "# test model after all epochs are completed\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data:\n",
    "            premises = batch['premises_input_ids']\n",
    "            hypothesis = batch['hypotheses_input_ids']\n",
    "            labels = torch.tensor(batch['label']) \n",
    "            outputs = model(premises, hypothesis)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())  \n",
    "    return all_preds, all_labels\n",
    "\n",
    "#evaluate the model on the test set\n",
    "test_preds, test_labels = evaluate_model(model, testing_data)\n",
    "\n",
    "#compute precision, recall, and F1 score\n",
    "print(classification_report(test_labels, test_preds, target_names=['entailment', 'contradiction', 'neutral']))\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 0, 0])\n",
      "tensor([2, 1, 0, 0])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(max_prediction)\n",
    "print(labels)\n",
    "print(torch.sum(max_prediction == labels).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(testing_data)*32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing\n",
    "\n",
    "Test the model on the testset. For each example in the test set, compute a prediction from the model (`entailment`, `contradiction` or `neutral`). Compute precision, recall, and F1 score for each label. **[10 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest a _baseline_ that we can compare our model against **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Baseline Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.37      0.34      0.35       344\n",
      "contradiction       0.33      0.35      0.34       327\n",
      "      neutral       0.33      0.33      0.33       329\n",
      "\n",
      "     accuracy                           0.34      1000\n",
      "    macro avg       0.34      0.34      0.34      1000\n",
      " weighted avg       0.34      0.34      0.34      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3  # Assuming three classes: entailment, contradiction, neutral\n",
    "num_samples = 1000  # Number of samples in the test set\n",
    "\n",
    "# Generate random predictions\n",
    "random_preds = np.random.randint(num_classes, size=num_samples)\n",
    "\n",
    "# Collect the true labels from the test set\n",
    "true_labels = []\n",
    "for batch in testing_data:\n",
    "    labels = torch.tensor(batch['label'])\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute precision, recall, and F1 score for the random baseline\n",
    "print(\"Random Baseline Classification Report:\")\n",
    "print(classification_report(true_labels, random_preds, target_names=['entailment', 'contradiction', 'neutral']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to the pretty strict label balance of the corpus, we opted for random labeling baseline instead of most common baseline. Based on the results, the baseline would be 35%\n",
    "#and our model is performing 20% better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways (other than using a baseline) in which we can analyse the models performance **[3 marks]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A qualitative analysis of some sort (referring to a manual examination of the labels in the dataset) might also give us insight into the way\n",
    "#the model is working and doing its predictions. \n",
    "#We can also compare our model's perfomance to other models used for similar NLI-tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways to improve the model **[3 marks]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can make the LSTM bidirectional instead of unidirectional, increase the hidden size and add another layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "\n",
    "[1] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). \n",
    "\n",
    "[2] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We met 3 times for 3-4 hours each time and everyone was present and contributed equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "This assignment has a total of 23 marks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
