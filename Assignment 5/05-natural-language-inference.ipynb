{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A6: Natural Language Inference using Neural Networks\n",
    "\n",
    "by Adam Ek, Bill Noble, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "\n",
    "In this lab we will work with neural networks for natural language inference. Our task is: given a premise sentence P and hypothesis H, what entailment relationship holds between them? Is H entailed by P, contradicted by P or neutral towards P?\n",
    "\n",
    "Given a sentence P, if H definitely describe something true given P then it is an **entailment**. If H describe something that's *maybe* true given P, it's **neutral**, and if H describe something that's definitely *false* given P it's a **contradiction**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore natural language inference using neural networks on the SNLI dataset, described in [1]. The dataset can be downloaded [here](https://nlp.stanford.edu/projects/snli/), but unfortunately that dataset is down at the moment. Instead, we will use the version uploaded to [HuggingFace](https://huggingface.co/datasets/stanfordnlp/snli) available through the `datasets` library. See the [documentation](https://huggingface.co/docs/datasets/v2.19.0/loading#hugging-face-hub) for loading a dataset from the HuggingFace hub.\n",
    "\n",
    "The (simplified) data is organized as follows:\n",
    "\n",
    "* Column 1: Premise\n",
    "* Column 2: Hypothesis\n",
    "* Column 3: Relation\n",
    "\n",
    "Like in the previous lab, we'll need to build a dataloader. You can adapt your code from the previous lab to the new dataset. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': 'A person on a horse jumps over a broken down airplane.', 'hypothesis': 'A person is training his horse for a competition.', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "#   LOADING OUR DATA - SNLI DATASET &\n",
    "#   DOING OUR IMPORTS:\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "dataset = load_dataset('stanfordnlp/snli')\n",
    "ex = dataset['train'][0]\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a0842dde6845fa83e4911fe18d24a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e507699e59425fba00676bf36f4432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b595e3b53d413eb2a80ecb20dab4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/550152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATING OUR DATASET FOR THE DATALOADER (will be easier since we are using a built-in dataset from HuggingFace):\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')   \n",
    "dataset = dataset.filter(lambda example: example[\"label\"] != -1)\n",
    "dataset.set_format(type='torch', columns=['premise', 'hypothesis', 'label'])\n",
    "# our datasets:\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "def collate_function(batch):\n",
    "    \"\"\"Merges a list of samples to form a mini-batch.\n",
    "    This function will be used as an argument for the DataLoader below.\"\"\"\n",
    "    batch_premise = [example['premise'] for example in batch]   # a list of 4 premises, since our batch size is 4\n",
    "    batch_hypothesis = [example['hypothesis'] for example in batch] # a list of 4 hypotheses, since our batch size is 4\n",
    "    batch_label = [example['label'] for example in batch] # a list of 4 labels (as tensors), since our batch size is 4\n",
    "\n",
    "    return {'premise': batch_premise, 'hypothesis': batch_hypothesis, 'label': batch_label}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dataset comes as a dictionary-like object with three splits: `'test'`, `'train'`, and `'validation'`. Each item is a dictionary containing a `'premise'`, `'hypothesis'`, and `'label'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': ['Playing an acoustic guitar, a young soloist performs.', 'A man riding a unicycle down a paved road', 'An older man is playing a violin on the street.', 'A cat is standing in front of a woman and a dog on a brick road.'], 'hypothesis': ['A young man playing the violin.', 'A clown rides a unicycle down the road.', 'A man plays music.', 'A cat is standing in front of a woman wanting a treat, and a dog on a brick road.'], 'label': [tensor(2), tensor(1), tensor(0), tensor(1)]}\n"
     ]
    }
   ],
   "source": [
    "# DATALOADERS FOR OUR TRAIN, VALIDATION AND TEST DATA:\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_function)\n",
    "dataloader_validation = DataLoader(validation_dataset, batch_size=4, shuffle=True, collate_fn=collate_function)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=collate_function)\n",
    "\n",
    "for batch in dataloader_train:\n",
    "    print(batch)\n",
    "    break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "This data does not come pre-tokenized. Instead of training our own tokenizer, we can use the BERT tokenizer like in the preivous assignment. Even though we aren't using BERT the tokenizer works with any model. See the documentation on [using a pretrained tokenizer](https://huggingface.co/docs/tokenizers/en/quicktour#using-a-pretrained-tokenizer). **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_word_to_idx(data_loader):\n",
    "    \"\"\"Goes through a dataloader's batches, transforms tokens into indices, creates a tensor out of the \n",
    "    premise and hypothesis. Labels are already tensors so they simply get appended into tokenized_batch as they are.\"\"\"\n",
    "    tokenized_batch = []  # tokenized batches get appended here\n",
    "    for batch in data_loader:\n",
    "        tokenized_premises = tokenizer(batch['premise'],padding='longest',truncation=True,return_tensors=\"pt\")\n",
    "        tokenized_hypotheses = tokenizer(batch['hypothesis'],padding='longest',truncation=True,return_tensors=\"pt\")\n",
    "        # Appending tokenized premise, hypothesis, and label to the tokenized batch\n",
    "        tokenized_batch.append({\n",
    "            'premises_input_ids': tokenized_premises['input_ids'],\n",
    "            'premises_attention_mask': tokenized_premises['attention_mask'],\n",
    "            #'premises_token_type_ids': tokenized_premises['token_type_ids'],\n",
    "            'hypotheses_input_ids': tokenized_hypotheses['input_ids'],\n",
    "            'hypotheses_attention_mask': tokenized_hypotheses['attention_mask'],\n",
    "            #'hypotheses_token_type_ids': tokenized_hypotheses['token_type_ids'],\n",
    "            'label': batch['label']\n",
    "            })\n",
    "\n",
    "    #print(f\"This is the [0] from the tokenized batch: {tokenized_batch[0]}\")        \n",
    "    return tokenized_batch\n",
    "\n",
    "training_data = tokenize_word_to_idx(dataloader_train)\n",
    "validation_data = tokenize_word_to_idx(dataloader_validation)\n",
    "testing_data = tokenize_word_to_idx(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premises_input_ids': tensor([[  101,  2048,  2273,  2893,  3201,  2000,  5607,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  2048,  3057, 11214, 22681,  2013,  1037,  2492,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  1037,  2177,  1997,  3157,  9567,  2006,  1037,  2754,  2024,\n",
      "          3491,  3173,  7334,  1998,  4147,  2304,  1998,  2417, 12703,  1012,\n",
      "           102],\n",
      "        [  101,  1037,  2450,  2003,  3030,  2006,  1996, 11996,  2559,  2012,\n",
      "          1037,  3696,  2006,  1037,  2311,  2813,  1012,   102,     0,     0,\n",
      "             0]]), 'premises_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]), 'hypotheses_input_ids': tensor([[  101,  2048,  2273,  2024,  2055,  2000,  5607,  1012,   102,     0],\n",
      "        [  101,  3057,  9334, 22681,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  9567,  2024,  2652,  5693,  1012,   102,     0,     0,     0],\n",
      "        [  101,  1996,  2450,  2003,  3061,  2006,  1996, 11996,  1012,   102]]), 'hypotheses_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'label': [tensor(0), tensor(1), tensor(0), tensor(0)]}\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll build the model for predicting the relationship between H and P.\n",
    "\n",
    "We will process each sentence using an LSTM. Then, we will construct some representation of the sentence. When we have a representation for H and P, we will combine them into one vector which we can use to predict the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model described in [2], the BiLSTM with max-pooling model. The procedure for the model is roughly:\n",
    "\n",
    "    1) Encode the Hypothesis and the Premise using one shared bidirectional LSTM (or two different LSTMS)\n",
    "    2) Perform max over the tokens in the premise and the hypothesis\n",
    "    3) Combine the encoded premise and encoded hypothesis into one representation\n",
    "    4) Predict the relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a representation of a sentence\n",
    "\n",
    "Let's first consider step 2 where we perform pooling. There is a builtin function in pytorch for this, but we'll implement it from scratch. \n",
    "\n",
    "Let's consider the general case, what we want to do for these methods is apply some function $f$ along dimension $i$, and we want to do this for all $i$'s. As an example we consider the matrix S with size ``(N, D)`` where N is the number of words and D the number of dimensions:\n",
    "\n",
    "$S = \\begin{bmatrix}\n",
    "    s_{11} & s_{12} & s_{13} & \\dots  & s_{1d} \\\\\n",
    "    s_{21} & s_{22} & s_{23} & \\dots  & s_{2d} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    s_{n1} & s_{n2} & s_{n3} & \\dots  & s_{nd}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "What we want to do is apply our function $f$ on each dimension, taking the input $s_{1d}, s_{2d}, ..., s_{nd}$ and generating the output $x_d$. \n",
    "\n",
    "You will implement the max pooling method. When performing max-pooling, $max$ will be the function which selects a _maximum_ value from a vector and $x$ is the output, thus for each dimension $d$ in our output $x$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_d = max(s_{1d}, s_{2d}, ..., s_{nd})\n",
    "\\end{equation}\n",
    "\n",
    "This operation will reduce a batch of size ``(batch_size, num_words, dimensions)`` to ``(batch_size, dimensions)`` meaning that we now have created a sentence representation based on the content of the representation at each token position. \n",
    "\n",
    "Create a function that takes as input a tensor of size ``(batch_size, num_words, dimensions)`` then performs max pooling and returns the result (the output should be of size: ```(batch_size, dimensions)```). [**4 Marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def max_pooling(input_tensor):\n",
    "  output_tensor,_ = torch.max(input_tensor, dim=1)\n",
    "  return output_tensor\n",
    "\n",
    "test_unpooled = torch.rand(32, 100, 512)\n",
    "test_pooled = max_pooling(test_unpooled)\n",
    "#print(test_pooled.size()) # should be torch.Size([32, 512])\n",
    "#for batch in training_data[:5]:\n",
    "#  print(batch['premises_input_ids'])\n",
    "#  print(max_pooling(batch['premises_input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining sentence representations\n",
    "\n",
    "Next, we need to combine the premise and hypothesis into one representation. We will do this by concatenating four tensors (the final size of our tensor $X$ should be ``(batch_size, 4d)`` where ``d`` is the number of dimensions that you use): \n",
    "\n",
    "$$X = [P; H; |P-H|; P \\cdot H]$$\n",
    "\n",
    "Here, what we do is concatenating P, H, P times H, and the absolute value of P minus H, then return the result.\n",
    "\n",
    "Implement the function. **[4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2048])\n"
     ]
    }
   ],
   "source": [
    "def combine_premise_and_hypothesis(hypothesis, premise):\n",
    "    output = torch.cat((premise, hypothesis, torch.abs(premise - hypothesis), premise * hypothesis), dim=1)\n",
    "    return output\n",
    "\n",
    "test_hypothesis = test_pooled.clone()\n",
    "test_premise = test_pooled.clone()\n",
    "test_combined = combine_premise_and_hypothesis(test_hypothesis, test_premise) \n",
    "print(test_combined.size()) # should be torch.Size([32, 2048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in training_data:\n",
    "  #print(batch['premises_input_ids'])\n",
    "  #print(batch['hypotheses_input_ids'])\n",
    "  premise = max_pooling(batch['premises_input_ids'])#.unsqueeze(0)\n",
    "  print(premise.shape)\n",
    "  hypo = max_pooling(batch['hypotheses_input_ids'])#.unsqueeze(0)\n",
    "  print(hypo.shape)\n",
    "  #print('premise',premise)\n",
    "  #print('hypo',hypo)\n",
    "  x=combine_premise_and_hypothesis(premise,hypo)\n",
    "  print('combined',x)\n",
    "  print('size',x.size())\n",
    "  break\n",
    "#With the batch size 4 the combined tensor should be (4,16) why isn't it working but the test pool seems correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "Finally, we can build the model according to the procedure given previously by using the functions we defined above. Additionaly, in the model you should use *dropout*. For efficiency purposes, it's acceptable to only train the model with either max or mean pooling. \n",
    "\n",
    "Implement the model [**8 marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, embedding_dim, labels_size):\n",
    "        super(SNLIModel,self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.classifier = nn.Linear(in_features=hidden_size*4, out_features=labels_size)\n",
    "        \n",
    "    def forward(self, premise, hypothesis):\n",
    "        p = self.embeddings(premise)\n",
    "        h = self.embeddings(hypothesis)\n",
    "        \n",
    "        p_out,_= self.rnn(p)\n",
    "        h_out,_ = self.rnn(h)\n",
    "\n",
    "        p_pooled = max_pooling(p_out)\n",
    "        h_pooled = max_pooling(h_out)\n",
    "        \n",
    "        ph_representation = combine_premise_and_hypothesis(h_pooled,p_pooled)\n",
    "        predictions = self.classifier(ph_representation)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SNLIModel(tokenizer.vocab_size,512,512,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, implement the training and testing of the model. SNLI can take a very long time to train, so I suggest you only run it for one or two epochs. **[10 marks]** \n",
    "\n",
    "**Tip for efficiency:** *when developing your model, try training and testing the model on one batch (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#print(outputs)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs,labels)\n\u001b[1;32m---> 22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     24\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "loss_function = CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(),lr=0.001)\n",
    "model = model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in training_data:\n",
    "        optimizer.zero_grad()\n",
    "        premises = batch['premises_input_ids']\n",
    "        hypothesis = batch['hypotheses_input_ids']\n",
    "        labels = torch.tensor(batch['label'])     \n",
    "        #print(labels)\n",
    "        outputs = model(premises,hypothesis)\n",
    "        #print(outputs)\n",
    "        loss = loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss : {total_loss/len(training_data)}')    \n",
    "# test model after all epochs are completed\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch in testing_data:\n",
    "        premises = batch['premises_input_ids']\n",
    "        hypothesis = batch['hypotheses_input_ids']\n",
    "        labels = torch.tensor(batch['label']) \n",
    "        outputs = model(premises,hypothesis)\n",
    "        max_prediction = torch.argmax(outputs,dim=1)\n",
    "        correct+=torch.sum(max_prediction == labels).item()\n",
    "    print(correct)\n",
    "print(\"accuracy:\", correct/(len(testing_data)*4))\n",
    "#print(len(testing_data))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing\n",
    "\n",
    "Test the model on the testset. For each example in the test set, compute a prediction from the model (`entailment`, `contradiction` or `neutral`). Compute precision, recall, and F1 score for each label. **[10 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest a _baseline_ that we can compare our model against **[2 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways (other than using a baseline) in which we can analyse the models performance **[3 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways to improve the model **[3 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "\n",
    "[1] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). \n",
    "\n",
    "[2] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "This assignment has a total of 23 marks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
