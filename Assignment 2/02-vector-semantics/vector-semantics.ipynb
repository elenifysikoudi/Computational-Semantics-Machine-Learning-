{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: Vector Semantics\n",
    "\n",
    "Nikolai Ilinykh, Mehdi Ghanimifard, Wafia Adouane and Simon Dobnik\n",
    "\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "In this lab we will look at how to build distributional semantic models from corpora and use semantic similarity captured by these models to do semantic tasks. We are also going to examine how different vector composition functions for vectors work in approximating semantic similarity of phrases when compared to human judgements.\n",
    "\n",
    "This lab uses code from `dist_erk.py` which contains functions similar to those shown in the lecture. You can use either functions to solve these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following command simply imports all the methods from that code.\n",
    "from dist_erk import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading a corpus\n",
    "\n",
    "To train a distributional model, we first need a sufficiently large collection of texts which contain different words used frequently enough in different contexts. Here we will use a section of the Wikipedia corpus `wikipedia.txt` stored in `wikipedia.zip`. This file has been borrowed from another lab by [Richard Johansson](http://www.cse.chalmers.se/~richajo/).\n",
    "\n",
    "When unpacked, the file is 151mb, hence if you are using the MLT servers you should store it in a temporary folder outside your home and adjust the `corpus_dir` path below. It may already exist in `/srv/data/computational-semantics/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = r'C:\\Users\\User\\MLT\\Computational Semantics\\Assignment 2\\02-vector-semantics\\wikipedia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a model\n",
    "\n",
    "Now you are ready to build the model.  \n",
    "Using the methods from the code imported above build three word matrices with 1000 dimensions as follows:  \n",
    "\n",
    "(i) with raw counts (saved to a variable `space_1k`);  \n",
    "(ii) with PPMI (`ppmispace_1k`);  \n",
    "(iii) with reduced dimensions SVD (`svdspace_1k`).  \n",
    "For the latter use `svddim=5`. **[5 marks]**\n",
    "\n",
    "Your task is to replace `...` with function calls. Functions are imported from `dist_erk.py`, and they similar to functions shown during the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file wikipedia.txt\n",
      "create count matrices\n",
      "reading file wikipedia.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1145485it [02:13, 8590.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppmi transform\n",
      "svd transform\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "numdims = 1000\n",
    "svddim = 5\n",
    "\n",
    "# Which words to use as targets and context words?\n",
    "# We need to count the words and keep only the N most frequent ones\n",
    "# Which function would you use here with which variable?\n",
    "#preprocessed_corpus = preprocess(corpus)\n",
    "#test_cooccurrences()\n",
    "ktw = do_word_count(corpus_dir,numdims)\n",
    "#print(ktw)\n",
    "\n",
    "wi = make_word_index(ktw)\n",
    "words_in_order = sorted(wi.keys(), key=lambda w:wi[w]) #is this correct??? why is it the same\n",
    "#print(words_in_order)\n",
    "\n",
    "# Create different spaces (the original matrix space, the ppmi space, the svd space)\n",
    "# Which functions with which arguments would you use here?\n",
    "print('create count matrices')\n",
    "space_1k = make_space(corpus_dir,wi,numdims)\n",
    "print('ppmi transform')\n",
    "ppmispace_1k = ppmi_transform(space_1k,wi)\n",
    "print('svd transform')\n",
    "svdspace_1k = svd_transform(ppmispace_1k,numdims,svddim)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house: [2551 3714 3104  567  962  627  443  185  311  189  131   28   93  169\n",
      "   81  125  151  408  194   89   79   29  217  184   62   15   31   70\n",
      "   10    1   41   21    1   31   37    1   30    5   25    7    3   20\n",
      "   11    1   32   36    2    5   65    4    0   46    8   18   28    0\n",
      "   20    7    8   16   10   40    0  175   10    2    7   19    1  174\n",
      "   11    3    1    6    0    0    0   10    9   11    7   24    4    4\n",
      "   14   23   58    7    0   10    2    3   10    6   18    6   13    3\n",
      "   22    0    3    5    3    7   14    3   40   20   19   15    6    8\n",
      "   23    4    5    1   19    0    3    1    0   14    0   14   53    7\n",
      "    7   11    6    5    5    4   12    6   53    1    1  433    4    0\n",
      "    5    7    7   12    1    1    3    4   17    8   16    1    2   31\n",
      "    1   12   14    1   44    6   14    9   38    7    2    6    8    1\n",
      "   10    6   10    1    9    7    9    4    3    9    0   11    3    2\n",
      "    0    2   11   37    2    0    2    1    5    9   10   16    4    6\n",
      "    0   21    1    1    0    2   47    3   27    7    0    2   13    1\n",
      "    2    0    5   31    0    1    0    3    9    0    1    0    3    3\n",
      "   17    1    1   16    3    7    4    7   15    4    0    0    2    5\n",
      "    0    2    0    5    0    9    0    0    8    0   10    0    0    0\n",
      "    2    0    1    3    1    3   15    1    9    0   19   14    0    0\n",
      "    3    2   18    3    1    3    2   19    5    2    4    1   10    6\n",
      "    0    3    3    6    4    2   25    4    6    3    1   25   10   15\n",
      "    3   10   15    1   10    1    8    1   13    1    2    9    9    1\n",
      "    4    1   25    0    4    6    5    5   36    0    2    2    2    0\n",
      "    0    2    3    3    0    1    4    6    5    0   50    2    5    2\n",
      "   14    6    2    2    4    1    9    4    5    3    1    0   12    3\n",
      "    3    2    2    0    0    1    4    7   12    5    0    2    1    2\n",
      "    3    4    7    3    5    0   27    7    1    1    0    3    3    3\n",
      "   10    0   14    2    0    2    4    6    0    5    0    0    1    1\n",
      "    4    1    1    0    0    0    0    3   20    0    0    2    1    5\n",
      "    3    8    3    5    1    2   66    1    2   19    2    1    3    3\n",
      "   21    5    4    2    2    0    4    3    5    0    7    1    6    1\n",
      "    3    3    1    0    3    0    2    0   89    2    3    1    1   14\n",
      "    0    2    1    9    2    3    2    4    2    0   25    0    0   23\n",
      "    0    6    2    1    3    0    2    5    0    4    4    3    0    4\n",
      "   58    3    1    6    2    4    3    3   11    1    1    1   10    0\n",
      "    7    3    1    6    1   18    1    0    4    2    0    8    5    2\n",
      "    0    0    0    0    5    1    2    1    1    3    1    2    1    1\n",
      "    0    6    1    4    1    3   20    1    0    5    2    5    2    1\n",
      "    0    0    0    2    6    1    1    0    1    1    1    0    0    3\n",
      "    3    0    0    6    6   74    3    0   13    5    2    2    1    5\n",
      "    3    3    1    7    4    0    0    2    3    0    4    0    4    1\n",
      "    0    2    5    2    1   14    2    0    0   19    0    1    2    1\n",
      "    0    3    2    0    0    3    1    3    3    2    7   18    7    6\n",
      "    6    0    1    9    1   10    2    0    2    0    2    4    0    0\n",
      "    1    2    0    1    0    2    0    0    0    2    0    2    2    0\n",
      "    3    2    2    0    0    1    2    3    1    1    1    2    0    0\n",
      "    3    0    7    2   39    0   14    0    1    1    0    1    5    3\n",
      "   11    0    3    0    1    1    0    0    1    9    2    1    0   11\n",
      "    1    3    7    0    0    0   32    1    0    0    0    1    1    3\n",
      "    0    9    0    2    0    1    3    2    6    0    3    0    0    2\n",
      "    3    0    1    0    1    4    0    0    1    1    0    0    5   21\n",
      "    2    1    1    3    0    1    7    1    3    4    0    5    3    0\n",
      "    7    2    0    4    2    0    2    1    4    4    0    0    0    5\n",
      "    3    2    2    0    4    0   23    2    2    2    4    0    1    0\n",
      "    4    0    3    5    3    0    8    0    1   16    1    2    2    7\n",
      "    0    0    1   11    1    0    4    0    1    0    1    2    1    5\n",
      "    0   97    0    2    0    3    0    8    1   14    4    9    2    3\n",
      "    1    1    0    3    4    0    5    1    5    2    0    0    0    2\n",
      "    1    2    1    1    1    1   12    0    2    5    1    0    0   13\n",
      "    2    0    0    0    2    2    0    0    3    1    1    1    1    0\n",
      "    1    2    1    0    0    0   10    0    1    0    1    1    1    1\n",
      "    0    1    0    0    3    2    5    0    0    2    1    0   23    0\n",
      "    0    4    0    1    0    0    0    1    1    2    1    0    1    0\n",
      "    0    4    1    0    1    1    5    1    1    0    1    0    0    0\n",
      "    1    0    0    2    2    3    0    1    0    4    3    3    1    4\n",
      "    0    0    0    6    1    2    1    0    5    3    0    0    1    2\n",
      "    0    5    0    0    2    1    1    4   15    0    0    1    1    3\n",
      "    1    0    1    4    1    1    2    8    1    3    0    0    0    0\n",
      "    1    3    2    1    0    1    0    2    0    0    0    0    1    1\n",
      "    0    1    3    7    0    0   42    4    0    1    2    3    1    0\n",
      "    1    3    2    0    0    1    0    0    0    4    2    0    0    8\n",
      "    2    0    1   15    0    0]\n"
     ]
    }
   ],
   "source": [
    "# now, to test the space, you can print vector representation for some words\n",
    "print('house:', space_1k['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oxford Advanced Dictionary has 185,000 words, hence 1,000 words is not representative. We trained a model with 10,000 words, and 50 dimensions on truncated SVD. All matrices are available in the folder `pretrained` of the `wikipedia.zip`file. These are `ktw_wikipediaktw.npy`, `raw_wikipediaktw.npy`, `ppmi_wikipediaktw.npy`, `svd50_wikipedia10k.npy`. Make sure they are in your path as we load them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numdims = 10000\n",
    "svddim = 50\n",
    "\n",
    "print('Please wait...')\n",
    "ktw_10k       = np.load('./wikipedia/pretrained/ktw_wikipediaktw.npy', allow_pickle=True)\n",
    "space_10k     = np.load('./wikipedia/pretrained/raw_wikipediaktw.npy', allow_pickle=True).all()\n",
    "ppmispace_10k = np.load('./wikipedia/pretrained/ppmi_wikipediaktw.npy', allow_pickle=True).all()\n",
    "svdspace_10k  = np.load('./wikipedia/pretrained/svd50_wikipedia10k.npy', allow_pickle=True).all()\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house: [2554 3774 3105 ...    0    0    0]\n",
      "house: [-1.63544799e+01  7.61827522e+00  8.60888633e-01 -2.62986171e+00\n",
      " -9.17158711e-02 -6.72420394e+00  6.50946557e+00  8.26802659e-02\n",
      "  1.73554823e+00  2.28500538e+00 -3.84902224e+00 -2.99752895e+00\n",
      " -3.11163158e+00  1.36996291e+00  1.50569453e+00 -3.63523269e+00\n",
      "  3.92398798e-01 -1.37867992e+00 -3.79506357e+00 -5.00300765e-01\n",
      "  5.79365264e+00  3.60842269e-01 -8.86147078e-01 -4.13688895e-01\n",
      " -2.28099521e+00  1.55104234e+00 -5.00316369e-01 -1.91359000e+00\n",
      "  1.44888685e-02 -8.44285607e-01 -5.48173627e-02  3.09304688e+00\n",
      " -6.45299176e-01  1.96314530e+00 -1.77570673e+00  2.95893614e-01\n",
      " -1.32387672e+00  1.68932385e-02  1.65285639e+00 -1.14233138e+00\n",
      "  4.49829010e-03  2.29213169e+00 -3.80991075e-01 -6.71369216e-01\n",
      " -4.43289406e-01  2.02489219e+00 -1.97391209e+00  9.75727469e-01\n",
      "  2.02781144e+00 -2.31833380e+00]\n"
     ]
    }
   ],
   "source": [
    "# testing semantic space\n",
    "print('house:', space_10k['house'])\n",
    "print('house:', svdspace_10k['house']) #will not be found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing semantic similarity\n",
    "\n",
    "The file `similarity_judgements.txt` contains 7,576 pairs of words and their lexical and visual similarities (based on the pictures) collected through crowd-sourcing using Mechanical Turk as described in [1]. The score range from 1 (highly dissimilar) to 5 (highly similar). Note: this is a different dataset from the phrase similarity dataset we discussed during the lecture [2]. You can find more details about how they were collected in the papers.\n",
    "\n",
    "The following code will transform similarity scores into a Python-friendly format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available words to test: 12\n",
      "number of available word pairs to test: 774\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [] # test suit word pairs\n",
    "semantic_similarity = [] \n",
    "visual_similarity = []\n",
    "test_vocab = set()\n",
    "\n",
    "for index, line in enumerate(open('similarity_judgements.txt')):\n",
    "    data = line.strip().split('\\t')\n",
    "    if index > 0 and len(data) == 3:\n",
    "        w1, w2 = tuple(data[0].split('#'))\n",
    "        # Checks if both words from each pair exist in the word matrix.\n",
    "        if w1 in ktw_10k and w2 in ktw_10k:\n",
    "            word_pairs.append((w1, w2))\n",
    "            test_vocab.update([w1, w2])\n",
    "            semantic_similarity.append(float(data[1]))\n",
    "            visual_similarity.append(float(data[2]))\n",
    "        \n",
    "print('number of available words to test:', len(test_vocab-(test_vocab-set(ktw))))\n",
    "print('number of available word pairs to test:', len(word_pairs))\n",
    "#print(word_pairs)\n",
    "#print(ktw_10k)\n",
    "#print(list(zip(word_pairs, visual_similarity, semantic_similarity)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test how the cosine similarity between vectors of each of the three spaces (normal space, ppmi, svd) compares with the human similarity judgements for the words in the similarity dataset. Which of the three spaces best approximates human judgements?\n",
    "\n",
    "For comparison of several scores, we can use [Spearman correlation coefficient](https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient) which is implemented in `scipy.stats.spearmanr` [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html). The values of the Sperman correlation coefficient range from -1, 0 to 1, where 0 indicates no correlation, 1 perfect correaltion and -1 negative correlation. Hence, the greater the number the better the similarity scores align. The p values tells us if the coefficient is statistically significant. For this to be the case, it must be less than or equal to $< 0.05$.\n",
    "\n",
    "Here is how you can calculate Pearson's correlation coefficient betweeen the scores of visual similarity and semantic similarity of the available words in the test suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual Similarity vs. Semantic Similarity:\n",
      "rho     = 0.7122\n",
      "p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "rho, pval = stats.spearmanr(semantic_similarity, visual_similarity)\n",
    "print(\"\"\"Visual Similarity vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the cosine similarity scores of all word pairs in an ordered list using all three matrices. **[6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space_10k cosine scores [0.8225343344374066, 0.8872057973594758, 0.8186279531466686, 0.8641170631512163, 0.9209106962592176, 0.8365560468285075, 0.9105543047663368, 0.839648226650919, 0.7589366871548553, 0.8729671430291857, 0.9196105130424984, 0.7823406678252945, 0.9385814369441279, 0.6712567699328355, 0.8119647954565936, 0.7762348891086438, 0.8757909279588578, 0.8334099693105823, 0.8498040003205741, 0.8010238123812328, 0.9329582307086745, 0.8685328881068068, 0.9016535452309239, 0.903411562467208, 0.8595810413395492, 0.9095905271212122, 0.9196059672024544, 0.9264745546789075, 0.8940138022865454, 0.6035018071770326, 0.9590760140404798, 0.505869075889037, 0.7958814635183578, 0.8579450500210568, 0.7585974002217747, 0.8377279786803276, 0.919494932764611, 0.8335758092457647, 0.9137889719993155, 0.852673659629823, 0.7440511994695072, 0.6028483799106399, 0.621440124024626, 0.8268897334463412, 0.6990249439362441, 0.9037427727220717, 0.7557653066169756, 0.9437849074877116, 0.8207956180814518, 0.8034503850990335, 0.8721266999122632, 0.8808716301935825, 0.861385674439392, 0.9072394472102073, 0.9147099135108685, 0.7955856790164796, 0.42684433404593347, 0.7858658993560472, 0.7623874637854213, 0.7956093583984004, 0.9448189745654407, 0.6931727548147454, 0.9577785097187591, 0.805738189037511, 0.8212256863695861, 0.8252640981150583, 0.9038102601998055, 0.9198643987006686, 0.8280140095336774, 0.8244045632740218, 0.8215648923632423, 0.9648117565815102, 0.9198160235076585, 0.7933541935040764, 0.9365134473727902, 0.9107657876516166, 0.9448131344642808, 0.9120057337003785, 0.7005465692100018, 0.6513769892440644, 0.9398908003026651, 0.8150642668112169, 0.900560690605104, 0.904103967824979, 0.8634876423522818, 0.8660154659868621, 0.9363009685656131, 0.7641074376676095, 0.8776089138062727, 0.4844535285369468, 0.9550719528891993, 0.8304648116821267, 0.8735130567370625, 0.8200177709601941, 0.8564068360309433, 0.9187925256806633, 0.7307517853964716, 0.9166804528049154, 0.8522751715240894, 0.8723535565685081, 0.7967653390107345, 0.7369030522832947, 0.8202050334052006, 0.9052996835201592, 0.7396217263764727, 0.896725694423791, 0.441997529614598, 0.5693222976223555, 0.9343589244055445, 0.9082746052465267, 0.7252265829405392, 0.7938898638832638, 0.7890160052970124, 0.8708261527564986, 0.8442084818652985, 0.9104593046721715, 0.9504419792251945, 0.866914207212514, 0.8638889630838567, 0.818912798391871, 0.9182566601206142, 0.9222017107281666, 0.6027321846286919, 0.8997596843030736, 0.8877721015143945, 0.8641291795339685, 0.9269637870577677, 0.8405844271128685, 0.8558436542077995, 0.9528308345834225, 0.5056490140340062, 0.7792611896171675, 0.870256277313881, 0.9145599955358537, 0.740921560160666, 0.7456043390122745, 0.7190580225267814, 0.9159079361874347, 0.8820550948075928, 0.8459813831520964, 0.8431702437787202, 0.894160668321959, 0.8248077870668982, 0.9541703063842157, 0.854783314934934, 0.8847833240034143, 0.8531038499512712, 0.745833711141815, 0.8223852261548167, 0.8045720314640014, 0.9156909256529119, 0.8513040123709809, 0.7884653676549483, 0.8799285672041598, 0.8355880139282058, 0.6292818147092618, 0.926744566967484, 0.7285325699468657, 0.8170017423234829, 0.7807779502211976, 0.8536071675351136, 0.899235519719185, 0.6689272146359182, 0.9576997110492192, 0.8121339435072924, 0.8573412046365197, 0.8397654723505477, 0.9201533400885953, 0.9002415660545788, 0.8821719099465886, 0.7366509885313783, 0.7993900323832573, 0.893711751652176, 0.8823379986100318, 0.9462654987218471, 0.8213970753984924, 0.6848877178377337, 0.9018566950920003, 0.9471839364617047, 0.9125694629649191, 0.4644225967679744, 0.81985515911313, 0.8644836631851671, 0.7642217101963962, 0.42666601008462046, 0.7447607609003056, 0.740955848481609, 0.7886799944847054, 0.8537638268652218, 0.7965633507766232, 0.8455524513194156, 0.901048880497932, 0.5939046863694907, 0.8524518810687589, 0.8014191529859613, 0.9115452002380421, 0.9367233565158452, 0.8801141037399691, 0.9248641525230815, 0.8871800162969129, 0.8732783051673343, 0.88245156365919, 0.9489244673114186, 0.8698587712031919, 0.7612282368770042, 0.8164603356065996, 0.8812937484050589, 0.9581319340010731, 0.878638492413167, 0.8341329334474228, 0.8326766412470162, 0.943916365463572, 0.9052780646214504, 0.8133554906651884, 0.8013091903871394, 0.8921335268089813, 0.8926328224567264, 0.7701820726946449, 0.9096117491366181, 0.8662808888381488, 0.827894611365709, 0.855819988836423, 0.7876015731361522, 0.8670048980011141, 0.9246752565231842, 0.8875969402777919, 0.8882341769921155, 0.7513053481073118, 0.753013918201522, 0.6982732004841076, 0.8915638972145512, 0.8393776142958853, 0.9041725071864442, 0.7786626506756777, 0.9056302710385, 0.4859009079882249, 0.9235358145883211, 0.7619203984411184, 0.8879684700977651, 0.8822574810287278, 0.7695209231799444, 0.8835918334751142, 0.9253819854017227, 0.9477552464270463, 0.9163144318192068, 0.7749949535229406, 0.8506456083787178, 0.7752048546176269, 0.5016701716666335, 0.7678400135185619, 0.8910902455490912, 0.9055586532330786, 0.7742632200773557, 0.7668639069304782, 0.832045706298929, 0.8534736172185754, 0.6800468072107526, 0.8349448853635704, 0.84880396374147, 0.6553420372675509, 0.899102253306242, 0.7577678695225001, 0.8739943629395086, 0.8560269602660903, 0.8677350927433971, 0.9235174364092822, 0.9260095902741433, 0.9706102088258755, 0.9015588488201568, 0.8293104962067926, 0.8325722816938571, 0.9475687539837292, 0.937732751056341, 0.7588328092347688, 0.7278057461090879, 0.9361628871895215, 0.8770985883763107, 0.46789645910667216, 0.6995825913930747, 0.7518836562133185, 0.8706822627628503, 0.8254971074691068, 0.805619563943941, 0.7509521592012941, 0.9156949817058785, 0.8862652859785676, 0.9088728991162945, 0.945664307174056, 0.8845165037200354, 0.4226513348003261, 0.8058355148010558, 0.847844929180617, 0.7817159825546846, 0.9279218016989705, 0.8919795397554147, 0.8739135543591315, 0.8456137167170414, 0.8059230667687196, 0.9257079364480973, 0.8528411268810269, 0.8516503149012912, 0.9066023234782015, 0.8699218923231979, 0.9206949270152183, 0.909125849546894, 0.895211025313507, 0.8230813837587894, 0.8811934951818859, 0.9358974429079003, 0.8419552983921301, 0.848061915760825, 0.9134521886224908, 0.926962875405551, 0.9640726721721975, 0.8579084783573704, 0.8859970413576581, 0.8347008599576342, 0.788439683025812, 0.8900861580199751, 0.8976441439437699, 0.9364307288555288, 0.8989928419655064, 0.7285547942868831, 0.7454257329996745, 0.890451325312609, 0.8523374805492431, 0.8131532102752304, 0.8964815317789812, 0.8032253236964345, 0.7438980741940906, 0.9281509328862911, 0.9605187459485972, 0.8250435089048356, 0.8879428721066873, 0.796896254465808, 0.919879758460288, 0.7508719700633295, 0.8321247516976462, 0.8503933885661149, 0.9034055310355585, 0.9032815289257642, 0.8995162469386778, 0.8639395863425006, 0.6662361480194248, 0.8902582449256431, 0.7771390837636567, 0.8119761613214707, 0.9287604633162769, 0.9254324213181543, 0.828227280576679, 0.8148409156040635, 0.72680961319802, 0.8762707770420939, 0.7786024220741911, 0.9188248725555526, 0.9320474459701914, 0.8533396506029445, 0.9580947425080697, 0.8053929445233263, 0.9098586445979456, 0.7680593729944536, 0.9341389880081854, 0.8089057934015973, 0.852260229104639, 0.9289257012068602, 0.8587122215486198, 0.7302431338438347, 0.8984988673924466, 0.8702663487070086, 0.863684193317283, 0.9566932997778411, 0.933669996571371, 0.9027974541173979, 0.9473960280756629, 0.8968546304315174, 0.9237754518623185, 0.8925131377330517, 0.8996684343726782, 0.9309441645193607, 0.7961071319591594, 0.8661261726535746, 0.8407037955351516, 0.8800242048802508, 0.9353098590829302, 0.821577277718834, 0.9367339513426629, 0.4943595926030035, 0.9372450012958871, 0.9015838282671826, 0.9484381494069775, 0.9213545499227681, 0.852466506043306, 0.7426973694397131, 0.9321263127227005, 0.764408719309986, 0.6971981921302594, 0.8986843857266995, 0.7295626684610237, 0.8764125641418639, 0.5356160207973656, 0.9483598249581161, 0.9305723037279737, 0.7467385414391233, 0.8219473775844761, 0.9693753497962351, 0.8658771331610656, 0.9229791100198387, 0.900484022759326, 0.9074132574176572, 0.888671255443235, 0.8282745008612847, 0.892643109396254, 0.8460327732797855, 0.8838958205988564, 0.8360687979989095, 0.7617668197032678, 0.8238031588576694, 0.851972663474814, 0.8654739223023874, 0.8919478494558285, 0.8579133872503574, 0.7317970382259839, 0.9036548422432183, 0.8786193346739205, 0.8485066021901572, 0.72298202010747, 0.897250579567251, 0.8779120783596979, 0.8133358110633884, 0.9050226769600693, 0.6316761107833664, 0.789761243208762, 0.9113757187908524, 0.952143548790671, 0.9192630465702952, 0.798403509696177, 0.90256706318532, 0.8651650160069092, 0.9071295548883667, 0.737166286633529, 0.9110755485678077, 0.8170041930233648, 0.8688771257437387, 0.8844361746044938, 0.8159711382449041, 0.8877857605063305, 0.8580626630681926, 0.7878952440038222, 0.7865474650691733, 0.4311692566154063, 0.9179830459695432, 0.42421634304718814, 0.8689361920986841, 0.7097411924500465, 0.7303502542217531, 0.8241518849361215, 0.8084269543563065, 0.6908608842813895, 0.8956239325515596, 0.581931813852423, 0.7885756732568209, 0.798273996013507, 0.9415081894202589, 0.8522830217103352, 0.7884844736939123, 0.9380841269701063, 0.8367765547708621, 0.8219893808593787, 0.8990837523519394, 0.7803003119078968, 0.837573127441842, 0.47895646421269156, 0.8738995358741699, 0.8643145472023591, 0.6317403636712481, 0.9030912133084879, 0.8994146370164953, 0.8845272296056212, 0.8830069621454086, 0.8506289136988451, 0.9666611746666499, 0.5034502784204702, 0.9075213356873671, 0.7697845342820734, 0.9190707341539718, 0.854337124552785, 0.9714573713363988, 0.9170631109495594, 0.8609951725784063, 0.909733406004045, 0.8949608777778942, 0.9299762324075833, 0.7576222182161687, 0.9271708111157798, 0.9604586194948257, 0.8198441292513947, 0.7706191571207102, 0.8649940572472875, 0.9229530611928056, 0.9187240624095989, 0.8868681583063929, 0.881260578050201, 0.8268351116390306, 0.9357640041065938, 0.6979394635991717, 0.7572476551122798, 0.9149510140592972, 0.8339804483475161, 0.7608244245475547, 0.6734819848222532, 0.7868986113281168, 0.7181580749848889, 0.8696889064236175, 0.8889625175339222, 0.7379708880476408, 0.8645939890118824, 0.9087443636816487, 0.906693727902773, 0.7855544147081163, 0.8764862040042553, 0.8954410766370015, 0.9200653682216553, 0.8190438381249597, 0.7078851999712126, 0.9542524274063086, 0.9520027430699175, 0.86539961446144, 0.9087867431235027, 0.7864578753580941, 0.9310998729118166, 0.8764207065695628, 0.8185818755345226, 0.8899084442643973, 0.8809116824165635, 0.7985297259866011, 0.874639074095624, 0.7549135055191928, 0.9379966208048622, 0.8513441659950799, 0.8734189953005288, 0.7400229998130375, 0.9414140544100376, 0.9148549555057557, 0.833225861620741, 0.8606301155646552, 0.8429091505230857, 0.850863617173372, 0.9123736532326491, 0.7544342552282989, 0.8812087443374634, 0.8710909251411788, 0.8719319101110192, 0.898952374129166, 0.8807715482480245, 0.9205361413831117, 0.8972103202944749, 0.8743057699621548, 0.808740119765091, 0.3927890842093858, 0.8192043833772906, 0.8217496826442838, 0.9509899564041246, 0.8150132045406456, 0.8578508142854224, 0.9157506898893979, 0.8431741849367546, 0.9171149471023855, 0.8878632638829039, 0.9142809898793137, 0.8921748095183837, 0.8260363355761223, 0.7611031176110428, 0.8540683703880166, 0.8105790278650705, 0.720121256614959, 0.8456379750176566, 0.8923446711192297, 0.6416937009579462, 0.9065372157377858, 0.6526665934013237, 0.8725188896509387, 0.8947668795974838, 0.8806365132622426, 0.6985592665238313, 0.7434691401159181, 0.8929847318108101, 0.9053468677767581, 0.48278107805407083, 0.8978799981704216, 0.8486180832684163, 0.8823800605806124, 0.4996672462216962, 0.8593796618711352, 0.8979822516943995, 0.9620124701053179, 0.7084181060038596, 0.9502129298152545, 0.7861983087113804, 0.8492152949307414, 0.8203488124922127, 0.9411519101286034, 0.47180915876366264, 0.8507801827133886, 0.7614617872025031, 0.8663990683773475, 0.8689412783665361, 0.7928624721544019, 0.907878422649513, 0.8719647143492707, 0.6078908320969597, 0.8658852761651399, 0.7539084177664819, 0.8130651982246553, 0.9183659295979193, 0.9423806128153167, 0.9409186477208312, 0.863664147814328, 0.7936138773316903, 0.6886820986134081, 0.9192524785829819, 0.7896246380860527, 0.8896665933507851, 0.7973687850548967, 0.9393658594823564, 0.8908636449569978, 0.836368952483825, 0.8559528197758683, 0.8188940814001916, 0.8975994602364775, 0.9477168962548027, 0.7920284926270817, 0.7202901860588623, 0.7532670424833555, 0.8780905223739557, 0.765521461421696, 0.8533724808154387, 0.9104040856114208, 0.7433228256167175, 0.8886816955147908, 0.7982490080317542, 0.8769414610541435, 0.849781697897128, 0.8969734297504128, 0.8107569528443715, 0.9316244500021467, 0.9289292626614047, 0.8601336127310837, 0.8108422971478344, 0.9328726340170379, 0.9372226440680618, 0.8464357614244216, 0.7102295140075692, 0.8138453627416633, 0.8674574027468368, 0.7661090656458971, 0.7507892416637623, 0.9114504214931789, 0.7942025413710394, 0.8246411702349995, 0.8802258076175707, 0.9561863989420728, 0.9138985727198202, 0.9316571474700998, 0.9229340513459748, 0.8415623617448564, 0.6808585404971638, 0.5342518638916611, 0.9388581840143184, 0.9359920522929773, 0.7424935230869043, 0.8473158243342603, 0.8814912522783384, 0.8843035210483247, 0.9205410434174904, 0.6931512799520139, 0.7645749150700802, 0.7353776714980527, 0.5092392576932749, 0.8375673610157162, 0.7772413126479374, 0.6477520094714833, 0.9318735276462803, 0.9354662027833952, 0.8762589687564272, 0.8826834771012778, 0.841801105922058, 0.8028001409041714, 0.783015675610918, 0.7474620737840587, 0.823296405351087, 0.8287191055581239, 0.9017946128288098, 0.8930222620837505, 0.8840329696363609, 0.9310557410448731, 0.8093249779974448, 0.903335300797996, 0.8759297368727507, 0.956106131775109, 0.9577039584960572, 0.9175933525753852, 0.9312393597670774, 0.8807014802018192, 0.6880665312038167, 0.8063280162795183, 0.8372985053750349, 0.9002493011872994, 0.7812738937954614, 0.9242875016840113, 0.508299894244805, 0.9265824205714195, 0.909962458557556, 0.9327416844534397, 0.7899712499760935, 0.8823983661431011, 0.9006683649941994, 0.9416279601093216, 0.8681042024982482, 0.8671615418419452, 0.8167035161258114, 0.49633178791539095, 0.9354991443914729, 0.6079635306988814, 0.8410808180996533, 0.9345901185217406, 0.8209924516695091, 0.8267156753634778, 0.8552813330375796, 0.8512596933104408, 0.9120332165319469, 0.9143644506593426, 0.9293927678179664, 0.7574240882043072, 0.8351246669938878, 0.8329523349461732, 0.8838021049565036, 0.8547745159553076, 0.9174823761149314, 0.9315813421867298, 0.8533695761125315, 0.8880352353295177, 0.9000062102124314, 0.8951447893204588, 0.9207628473717837, 0.8779878828881148, 0.8732946874292874, 0.9121433155170986, 0.8258583173774884, 0.9088373120379164, 0.7991376997945473, 0.9181520054609124, 0.7142005316063685, 0.8159761961537351, 0.8464652925685651, 0.898391842588108, 0.7745292517913239, 0.8707847493413405, 0.7702457997997131, 0.9095815642470393, 0.8400385610367218, 0.9378592654936035, 0.8907934362110993, 0.8800601881528239, 0.8696148153859021, 0.9588685890100145, 0.8729174520977399, 0.7919688171444057, 0.8763147076381798, 0.9009906841286136, 0.6423848155094528, 0.8737740545437964, 0.8726165199678153, 0.9264959393667596, 0.7628528775304785]\n",
      "ppmispace_10k cosine scores [0.07670650423741827, 0.03674757699351012, 0.026098999599578045, 0.060056204366473645, 0.030136518663719275, 0.05526210787679181, 0.1464650816483296, 0.04527229880437765, 0.049543818307671635, 0.03429479455580578, 0.08514042815257318, 0.05279861325966581, 0.09030912312783426, 0.11626316979993921, 0.07362124811628401, 0.04889841513452965, 0.0736666597001374, 0.0723858153861059, 0.07882944861452362, 0.05773104642616836, 0.08594886845085606, 0.027499159263544362, 0.0717881468391875, 0.15753338499262037, 0.10327246254504681, 0.14549993849788018, 0.2682277189457574, 0.06298576106252575, 0.061027143302488714, 0.01726787278305841, 0.06492768259243888, 0.03127695997437622, 0.029642372970768206, 0.03915712963868586, 0.06348960428886366, 0.05743805871723653, 0.0349965594256613, 0.11360162017365635, 0.06831603822720073, 0.06065689540938394, 0.05529595316008414, 0.003157648323901167, 0.022004113856504047, 0.035225334429990984, 0.04775363990000067, 0.08386880715093573, 0.06258992887047089, 0.06977289974894475, 0.07559063634674802, 0.0772406363954339, 0.13066438618766793, 0.06643323303906785, 0.15500055312235364, 0.02558223320259153, 0.034322068494043595, 0.0324878176282838, 0.034652761727155076, 0.0463939866654628, 0.03216816665339205, 0.06284244403529786, 0.05569520525464976, 0.03897927145566359, 0.20085046069169432, 0.020023725281694393, 0.018887702359643026, 0.05775629129594116, 0.026263568163252187, 0.059378177800328474, 0.08145164989359026, 0.01153886385115207, 0.08523768141862735, 0.15212213984146716, 0.07100797915884467, 0.028908271912538506, 0.08671861209742487, 0.04591742004796852, 0.06478731426376444, 0.07313895217165424, 0.043182495959920034, 0.012445964015693624, 0.10414340449538736, 0.07437696287241993, 0.09220233764272368, 0.10894693120256127, 0.06113711521602131, 0.07482259561341048, 0.11946664011919488, 0.0388271224173293, 0.18522329851141975, 0.03447951164259132, 0.046932918515924904, 0.031756826758982316, 0.11311806103881786, 0.04547283276389416, 0.06619430027353701, 0.16671232887501441, 0.05525106561557487, 0.12309002475740176, 0.06278264383669833, 0.07238030499838086, 0.07154365527156113, 0.02668118740930264, 0.07557651245503637, 0.07744473465250465, 0.020681854680239715, 0.2419181038750236, 0.06797873195899491, 0.020546263012285015, 0.15487827778314386, 0.09088654908070772, 0.0433002737954182, 0.04126265137038297, 0.04546684227524921, 0.038506120263918135, 0.07403156571657589, 0.10188956319203621, 0.174977883334559, 0.07663324674983941, 0.05223109169479952, 0.046204194216298046, 0.19193580230767593, 0.015847057297494794, 0.04576385096213499, 0.02474816012743857, 0.046392014922996536, 0.19021178766104524, 0.2667117529684212, 0.030243749185834005, 0.06247814599706751, 0.03518695288721688, 0.07846543880048859, 0.054045780467871, 0.049798379821701454, 0.10085270289775339, 0.08689429821666521, 0.06807486511312705, 0.10460479082152921, 0.17127786943742782, 0.03963131352298028, 0.01750080724559744, 0.1407413659236465, 0.03769914329626367, 0.041800801362129075, 0.0578497978943819, 0.10830031613793421, 0.08921697314506, 0.07145658975578172, 0.029456049170716286, 0.091769776483652, 0.15506167225738793, 0.04877117429322367, 0.04800447377838415, 0.032089127944597236, 0.06413679837882466, 0.017198405334978642, 0.029402261183086056, 0.21308440798428155, 0.06826685143381515, 0.06237268250427828, 0.051510973993595915, 0.032367731224594964, 0.06255038384648212, 0.02172793859944363, 0.1536454068419797, 0.03858842027467041, 0.13519421939615078, 0.03137404841877872, 0.20080951471753686, 0.0456689504109843, 0.06371905422342378, 0.02655002868292611, 0.12197236013378998, 0.03275396275580144, 0.04128041042340741, 0.02355192908314028, 0.04597024413448268, 0.04386130638982056, 0.049599166880296655, 0.08098958334283993, 0.022993807829009026, 0.08934387559413365, 0.012593684180384047, 0.044749460458967216, 0.02650915334874613, 0.05494343513558273, 0.04674908898427987, 0.048521528540106065, 0.06452732218235462, 0.08438353410927112, 0.08761867078012406, 0.09858218291360457, 0.11101730641386706, 0.019562409479938307, 0.03913616489597653, 0.06839339566349119, 0.0396581734954189, 0.03430211705303439, 0.17122991931246567, 0.03415358294059478, 0.043463954886537134, 0.015007803110079238, 0.1203036489010614, 0.2031113460455517, 0.07081047925475865, 0.03189332482491696, 0.049218750192582296, 0.06479726995723228, 0.12870018028367072, 0.04349609517596469, 0.0419651112233909, 0.03225250712818818, 0.2229650793635755, 0.06307273697391405, 0.07644744049000389, 0.049356620217217755, 0.03664235436303769, 0.06164636468681044, 0.024553870569770354, 0.18305109110576331, 0.05344989447000206, 0.05181182642265167, 0.0901847149014727, 0.05443428438088431, 0.042118204122308135, 0.06562916557189949, 0.05670315151820223, 0.08547416571653856, 0.04859899863748416, 0.04383823448221678, 0.05371691771067904, 0.021927177475610737, 0.0471190821936041, 0.10412134347887601, 0.036348602490764166, 0.02456097552065622, 0.05016527608631656, 0.09432757512746734, 0.023379027136083162, 0.09871277397658784, 0.07114241485529323, 0.03860749552611992, 0.034465785666165524, 0.10480144635924166, 0.044690372122397654, 0.05561482923009742, 0.06608649783714966, 0.05924747560308009, 0.11728913379287234, 0.04803736727408283, 0.049196624625316786, 0.17242313799085873, 0.07788563645467399, 0.04316345143882559, 0.03955721030656514, 0.05460040852352496, 0.09527181166375064, 0.029525855672072932, 0.039890864219487565, 0.037172949693426445, 0.04474986448881789, 0.08411314701015833, 0.0417103099639008, 0.035535306110111346, 0.06899731232451436, 0.04847219073326354, 0.10824761647437431, 0.030244871595163453, 0.15452862430298758, 0.039977800243849254, 0.08084750107651967, 0.05907209089296485, 0.10305968078779033, 0.10677544437677663, 0.1253331140451982, 0.05809694351700577, 0.25151127696609227, 0.11075261526291544, 0.03144557427611974, 0.04082695746894573, 0.034790305764540945, 0.01709905892184211, 0.046205686703698366, 0.025449484792155367, 0.03080481869314622, 0.06506088624611095, 0.07749699712262846, 0.12593973625378158, 0.07300639448907625, 0.03269233244209023, 0.04654529095849744, 0.04467045834752656, 0.05647799697353851, 0.04440271171456809, 0.09315364143251709, 0.08233870164534261, 0.0904691936730899, 0.02827558670032277, 0.04821888773583102, 0.12171061767032153, 0.09629963606931372, 0.1471423075878733, 0.05886311436650599, 0.05142047110374054, 0.03943763356728354, 0.06640299000441544, 0.060206435034011886, 0.056063308088056464, 0.08079512839799638, 0.15642661241088313, 0.12438064776253968, 0.12142203010561407, 0.09217794888617267, 0.025782769769680684, 0.23662640700111479, 0.08913575397251453, 0.04314559728483977, 0.14258304856375542, 0.03214043758057851, 0.061324127643945084, 0.05052412017880306, 0.06577113330185098, 0.020677518878632895, 0.022380595852735066, 0.046091270742739866, 0.11143701456137195, 0.04285872120824871, 0.053685709445163356, 0.0994652674314591, 0.023497700408924793, 0.06440222460882988, 0.09475174589946381, 0.10663372434070759, 0.08508574795080512, 0.03827375276576627, 0.045794989953625034, 0.06194799396980832, 0.03668753465298552, 0.06994167261723845, 0.06127386076332775, 0.09233895201886243, 0.0379081975979209, 0.06668706201939417, 0.0363577874706203, 0.0304854973179914, 0.08542967864628453, 0.10183228553840255, 0.058246946014962685, 0.06482473390813406, 0.044460948826798186, 0.04554911650462337, 0.02554890035854472, 0.037905437847468255, 0.045220600424408564, 0.0496485407965425, 0.05659939220834479, 0.04242734469587676, 0.07861007370901187, 0.18139213138944987, 0.007498754228989402, 0.08223186325713977, 0.051107390454815174, 0.07282895201498653, 0.046335078912516196, 0.04384000639325119, 0.15844063562941013, 0.04604418297597671, 0.03232361518468364, 0.027193105885578012, 0.08329251985035786, 0.13096864268155053, 0.09420708186844948, 0.0682027843294565, 0.12034004107558978, 0.2541415540049444, 0.04872551664582002, 0.11149604585499397, 0.12359993869848496, 0.04878463373667985, 0.04452164338124655, 0.07899340141125184, 0.11411238506134135, 0.0402550948852582, 0.05368888973600158, 0.1120956171436683, 0.026713029514312994, 0.04210830056140415, 0.07274274698156381, 0.26170579646580633, 0.1481542927154383, 0.0781875750717068, 0.03793044616157773, 0.08991450118999031, 0.018596767036528798, 0.12148016338908835, 0.0433792313805153, 0.04989077141568189, 0.10167467288032485, 0.033570799120579586, 0.10174277440050408, 0.03891704073128996, 0.07483134839379434, 0.22272580921922663, 0.028293560142804954, 0.10308063041081134, 0.06588465334052312, 0.04883605742719912, 0.1092438417393102, 0.19802948965269512, 0.07770065654759453, 0.07903949327120316, 0.056602127395787946, 0.06038982358413667, 0.03592095693616791, 0.03730008926810593, 0.08121196894166122, 0.02882248945868164, 0.02418854242915933, 0.07095608758083541, 0.07255139108770015, 0.044845406759699766, 0.054080049674954264, 0.06342518736269454, 0.056436980283801036, 0.04522910681679643, 0.02475298266517404, 0.024281880078305635, 0.061842070865783, 0.03471746055500693, 0.06407558299463588, 0.10943617092071353, 0.04754725070583874, 0.05598759960771515, 0.15969022500562105, 0.15368631533369645, 0.08156806443347059, 0.05328619347442699, 0.05151798283256013, 0.0987630755567801, 0.04531596472436423, 0.036026467373787126, 0.13138559174442752, 0.04594752082211721, 0.04422906262634282, 0.07034075685041898, 0.05250880341376223, 0.10135947863901819, 0.10210493459541452, 0.06858961130133906, 0.0668952768098569, 0.03645254805720095, 0.11528326765644506, 0.035204620871548936, 0.13289276345215956, 0.043447885711830926, 0.016313001912182302, 0.0685766259586002, 0.08086366205050263, 0.08609907545775973, 0.04773938865920704, 0.020351543315265726, 0.07136481018861288, 0.04840281382780054, 0.09158207695357459, 0.07225824721272193, 0.05963151785533173, 0.09167010703903511, 0.07997199890626745, 0.03198341570721195, 0.04439107164449672, 0.1304528210834237, 0.034531366262671635, 0.014231551937884784, 0.038899567082405155, 0.07678667890545887, 0.044888502804619645, 0.040954236809593425, 0.0836788194111569, 0.1383027309123845, 0.05183444166503492, 0.057142591096519986, 0.18131324826838327, 0.004221555141898735, 0.0445950741385789, 0.030696061630767983, 0.04814402642518076, 0.0966329170219328, 0.21544983926167643, 0.08605200506448611, 0.056568978989422855, 0.06005176815419262, 0.042306103579484736, 0.05665418911009558, 0.04576999127666065, 0.07451942407324091, 0.11498463022469559, 0.024927783080316023, 0.055665100149047804, 0.07235706475076759, 0.029534238227887463, 0.07570794349562598, 0.04482597269026679, 0.03985994846066, 0.03275079626746114, 0.07594081302995778, 0.02118322837383229, 0.04021774825078626, 0.15495988045736747, 0.09328824505022601, 0.030757481203076552, 0.09562393727775106, 0.061944665900359394, 0.06223173110639236, 0.07148581487306689, 0.04391158971483218, 0.0673754891449413, 0.1287528045658472, 0.1582959763874966, 0.043912668342867174, 0.028369916448309787, 0.03989338422355557, 0.020018614715937653, 0.20255130401877647, 0.05956627452297097, 0.06270498677030742, 0.0838459336913692, 0.14424190016221908, 0.017792475277369182, 0.08493282833796136, 0.026194243973290777, 0.06500225936265579, 0.06505374004735685, 0.09982083137920131, 0.06764804556101774, 0.09480232377817822, 0.04285606785738229, 0.05222259155029373, 0.018138781485806173, 0.03883995270463464, 0.05058510521361765, 0.11243340029750051, 0.06651461251900385, 0.12242825709874255, 0.11541521307590373, 0.08814912414492143, 0.056973536358012666, 0.08698285897638285, 0.043055335888910556, 0.033320973693078015, 0.0570200794389562, 0.12271683961476924, 0.0375942639973678, 0.0677498215150232, 0.029828775908515844, 0.10282687091630785, 0.12409084823516767, 0.13477953306422064, 0.12220974964558304, 0.05679188342821416, 0.10984603510067327, 0.08993630823579493, 0.05461294642021561, 0.18506608984593315, 0.0387465798782221, 0.06098685850139475, 0.07962563640303658, 0.06020665035081546, 0.21387990528224685, 0.07536515093948846, 0.07752966449432981, 0.04802505655604382, 0.037908219473814266, 0.1040129835557943, 0.06695177262959327, 0.01732091154982271, 0.03935882403232706, 0.06747020902267112, 0.23152967954405246, 0.11030433316842729, 0.06802402802186398, 0.055977778920464905, 0.0771927425318956, 0.048790384324771986, 0.06071702540657328, 0.020505790135852804, 0.04194057242680014, 0.08001885082990898, 0.06476327486934416, 0.03024349977929843, 0.08049957661559277, 0.08005541367439496, 0.04353431047547019, 0.052816609232819035, 0.04641844124234877, 0.203168454085151, 0.32313578425223405, 0.03823129165584282, 0.05882054315516407, 0.045806447812123716, 0.017615448673516027, 0.06021023660717015, 0.04289817800737608, 0.030367880412052288, 0.10065439016494768, 0.01695759767973825, 0.052373202912113885, 0.02534704455827972, 0.06560067011208653, 0.09109543980332355, 0.154577350609247, 0.030427572693696316, 0.0831529238848997, 0.021157487858369135, 0.0394867835625229, 0.14465816213635674, 0.056831588178326536, 0.06702676943462052, 0.20607217448323234, 0.06548143587487473, 0.06487489584874517, 0.1915161612614377, 0.09318150275792178, 0.09850303954200075, 0.055679789561072404, 0.19287809749509982, 0.06536442380784448, 0.06075745916673087, 0.06146258094246297, 0.038586469148721415, 0.04293318304249625, 0.1367982011942835, 0.0465943502899199, 0.05518634918990539, 0.03401338515796601, 0.08295708021886133, 0.06743289963322913, 0.027111880026902506, 0.025923721100746484, 0.05122848833901173, 0.23157228767285556, 0.07328719672826232, 0.058216462525079324, 0.056186092337574364, 0.16069436702474005, 0.04952912750255476, 0.1283218067119966, 0.04148411102561257, 0.0823313257494095, 0.18696406784805775, 0.08540391728929829, 0.16214661745408726, 0.04797650974710533, 0.024536642565571358, 0.06559973768617117, 0.05150539426312836, 0.08564970634810147, 0.05007430426818926, 0.04113674393755369, 0.06105506136185085, 0.03871201765257276, 0.04942191223607688, 0.20517773109293677, 0.05947843417442304, 0.06522562166175602, 0.15212311251928723, 0.029875732690131905, 0.05394754921049225, 0.09800679814458657, 0.15982082235849626, 0.06457763657802601, 0.10351245885582805, 0.0283833303704751, 0.05048516470454554, 0.06553414349647885, 0.054974256891310434, 0.0629749591594484, 0.04233483074323599, 0.045350406434862836, 0.07236069797509434, 0.026922790846570822, 0.021004191462347915, 0.02472095311283709, 0.05791609111476649, 0.07079230029840444, 0.09947586781576245, 0.12486706379771853, 0.05672794123840989, 0.0917825294226374, 0.03399234725008382, 0.04698071591025, 0.030419876194184535, 0.07234602499874808, 0.02531351467997306, 0.05376482708166809, 0.04991056268524918, 0.09773415748858012, 0.047934092064244774, 0.05823187303385663, 0.05540609449164147, 0.11014347777099126, 0.11235493289640594, 0.08400356867743192, 0.04146655846056099, 0.04665716503571704, 0.02906986655114471, 0.03023890097441594, 0.05350566608633415, 0.07938476413454193, 0.044117831590357655, 0.05457507565727192, 0.06123963053067437, 0.02803802448714279, 0.1498577949887728, 0.049233330486672625, 0.03901328095139611, 0.0917802684136688, 0.06823922163766827, 0.05882924069976423, 0.07460333838202879, 0.10147385144168897, 0.022880587689239052, 0.017422400311744524, 0.1765182867864823, 0.006243236298652295, 0.11338256154733761, 0.13011546776598232, 0.08225280208573552, 0.11298374822124231, 0.06908165394871076, 0.041946471433795346, 0.0828215110758219, 0.09939276497840245, 0.05879081542276819, 0.062285083518780954, 0.08136186966422834, 0.04206673881615113, 0.07113856428688262, 0.021541265370744467, 0.1611017643213941, 0.0332929364976827, 0.05293855396672958, 0.022232295491301683, 0.10544600310921812, 0.11562055264380024, 0.045203965563170176, 0.06299969983406978, 0.08136491169990276, 0.06691245485283442, 0.05079201673921624, 0.09122464739792976, 0.05683779479292546, 0.08567344411669467, 0.12024779771892331, 0.08798747074598699, 0.050670104499651675, 0.1439065299243796, 0.06517074778347795, 0.1747682941153824, 0.07027621799676503, 0.08059592630611728, 0.11486094746554462, 0.08242884379800006, 0.05711184984058809, 0.09924257662755988, 0.11875047155810312, 0.0839957708922833, 0.058283357840579614, 0.054141432633148814, 0.03349586672129108, 0.12378923402934552, 0.014785875789145413, 0.050189418532395454, 0.03811108735700805, 0.05790530622700855, 0.03301936794773643]\n",
      "svdspace_10k cosine scores [0.7359976058155725, 0.3983177634025829, 0.3722460644541822, 0.5975661685955933, 0.3690306386325187, 0.5893415465191162, 0.9193978802980551, 0.49439357366811376, 0.5508317104291022, 0.4488179731487246, 0.7480447749341349, 0.792627413360085, 0.6765989522808169, 0.6067415043142642, 0.585828143531164, 0.693249587813882, 0.4521661820650984, 0.5208864753655356, 0.3293908378861941, 0.2444610045344937, 0.5549712220995381, 0.40776664107004057, 0.7368842109295529, 0.6539220051675052, 0.8507899835626389, 0.8738469340228833, 0.8945878123044746, 0.44000745256054374, 0.37795260739412634, 0.3082773600864455, 0.5093199594890686, 0.30708919164476556, 0.25679094422540993, 0.5273721550226151, 0.5055453647956529, 0.4633414285403253, 0.6920328370018607, 0.6072560755854924, 0.23053313079328852, 0.6742501200660076, 0.45912189593193586, 0.21211176779003707, 0.2305668557906742, 0.3134234980879197, 0.4277654754218142, 0.7074834507658692, 0.6092794853606531, 0.7782590569773072, 0.528732064990201, 0.5748280240992898, 0.6815850738195728, 0.7779878509006768, 0.6958246189238221, 0.12320650820006293, 0.3691701867135042, 0.43699839255419853, 0.4572264524759492, 0.4657072708232926, 0.2729401893625749, 0.5760008551246282, 0.7837827438022217, 0.3273604755445634, 0.8589233576572611, 0.5633974654881252, 0.2588725854384832, 0.48245718679257843, 0.19954340352469802, 0.6258464648836967, 0.6673735984900974, 0.22715994409418347, 0.6017345783286412, 0.8963502409871646, 0.2396963814303472, 0.3742389524676015, 0.7500501035636189, 0.5009539176789286, 0.6847778402956328, 0.6237685571628768, 0.1918038201170092, 0.379990903662165, 0.5325679714610513, 0.6016009304085643, 0.5676972385443738, 0.41956468676287373, 0.32151882979017177, 0.6445381393604375, 0.734914594672452, 0.420716940868193, 0.8383634790886195, 0.5952083699173346, 0.42979575898281513, 0.6404316071390843, 0.8930215904319312, 0.3450806418447118, 0.4967908624572943, 0.753808731820201, 0.4612776692941909, 0.7959210556282008, 0.5552941428001631, 0.35320780036120103, 0.6958434420786062, 0.23766834683285493, 0.470305134908421, 0.7048255973853221, 0.6188308883109142, 0.9154635093237169, 0.563699069750188, 0.29050078897178194, 0.667325652567871, 0.5939737215080747, 0.5731492800775804, 0.3217336099648975, 0.31132424750004795, 0.5084891577051214, 0.5284860596216558, 0.7235254796339595, 0.8244728336000657, 0.48500429502015563, 0.5946084048555214, 0.5379826225212926, 0.7836829901448561, 0.23663560138344178, 0.45015319563105216, 0.298173365932317, 0.236863958336745, 0.6959061090106138, 0.8879056818440111, 0.18518206179415558, 0.46648329370795766, 0.5249825765215528, 0.6429941804983261, 0.3857037174996369, 0.4174493322988997, 0.5783414230467007, 0.329684029681503, 0.3324669465722018, 0.3957297635287494, 0.8660534321431942, 0.6146732157538795, 0.42671529902236127, 0.7981952613682097, 0.31219472108828744, 0.3224867238096061, 0.7210280656253113, 0.34632115774102584, 0.6713932052772238, 0.4246254507981189, 0.30599556675288064, 0.5847245468123659, 0.7318324711626794, 0.6650811488385664, 0.41291838830945776, 0.3309560642262483, 0.45467176679940763, 0.6747465367527512, 0.23078846977499917, 0.8946515494499254, 0.5686868912324907, 0.6033616994610829, 0.2536305136299603, 0.20104209806735487, 0.7956879897628683, 0.3290075370160152, 0.5944561791583621, 0.3451750925061652, 0.8374801238639877, 0.3173446551053253, 0.7153219007307774, 0.401943381423267, 0.754681145837994, 0.24133386516933256, 0.8902285638016398, 0.5794755781181662, 0.7300472808871092, 0.7029356271474785, 0.2649297441221756, 0.5622644182909902, 0.7336188800096433, 0.5936360167623054, 0.2892879564199668, 0.7228410202167888, 0.3719952537245478, 0.5451618124089638, 0.3086656179345369, 0.6346755283666068, 0.2705600325186221, 0.6770732305101984, 0.39837902160329414, 0.7624488691928897, 0.5309610041147926, 0.7345239713039321, 0.9024812852064323, 0.13052891676910386, 0.36849454684030325, 0.743410852312719, 0.6621047200995648, 0.5998746443950205, 0.8187137928790723, 0.4023830652636398, 0.6376919696271153, 0.41000104465614134, 0.647860492004985, 0.8158129209436328, 0.60592987826197, 0.47907136387995186, 0.2907107392839697, 0.4887318047286255, 0.7316499580941465, 0.6480139090279691, 0.6629296918283315, 0.7067413797760983, 0.910845388968883, 0.4191106464267311, 0.7056034502017113, 0.6358957062271182, 0.31729805176289794, 0.7653691301610076, 0.25892363882837044, 0.8288981029167458, 0.4355939162485357, 0.5296171722720613, 0.7707168404685681, 0.5648061003247492, 0.5491248790128311, 0.40910750459919265, 0.37807963138632744, 0.7416107073403067, 0.36659645216124154, 0.5791142931411514, 0.30043579613384985, 0.27018433780244694, 0.5201613247010424, 0.7006730577806547, 0.6876474487976346, 0.5355510855555387, 0.4895438354649704, 0.4263506659338791, 0.16561503170165312, 0.7427080405065801, 0.7114529195089994, 0.38276355775003834, 0.5580137923054451, 0.6166372387116791, 0.5079882952647724, 0.2902189029681992, 0.6704494118041276, 0.5332825493540883, 0.8542406977713897, 0.5306093913516201, 0.2537724625491608, 0.7701416434089556, 0.5178949273466745, 0.48772320869236163, 0.4660061663417411, 0.3820449673104562, 0.49132068426261694, 0.32952752958027964, 0.23993733881184126, 0.2020422383608905, 0.658440921117791, 0.5567250941385269, 0.5712846153857635, 0.39408705949715117, 0.4889338406186001, 0.4409459634794878, 0.7723244985334595, 0.6810000180430749, 0.5567608261112622, 0.6325718361093285, 0.7839202695938412, 0.8271267163016984, 0.7198872959400079, 0.7134924754649858, 0.8856660931295347, 0.49234135874380647, 0.9417434439268206, 0.7637885128681778, 0.2688118944387758, 0.2335329887696421, 0.14761600284575183, 0.6753363684214964, 0.5792143489802533, 0.4816076835565601, 0.21971947803685843, 0.5059777602263597, 0.7567953299891135, 0.611134052948266, 0.6038377694466353, 0.6424588761580093, 0.44385441500614564, 0.4631677384297054, 0.7001219404514547, 0.25350355534507624, 0.5769243311718335, 0.4056374327664346, 0.62120499127984, 0.27336465061329446, 0.5063740939420452, 0.8574482106157475, 0.33179294868326975, 0.7994663673533327, 0.8391006436284189, 0.7053309221790646, 0.6769500753885975, 0.37470499484527825, 0.3323711711780614, 0.2347316225289296, 0.6540851756727208, 0.8484008464634756, 0.6586862388660003, 0.8334050853793549, 0.548601868077068, 0.5152714773439004, 0.751476824050534, 0.3947590646053008, 0.35516488152032705, 0.7376249241127881, 0.5159585293948341, 0.3366163919223606, 0.6281051345879409, 0.6333686056301603, 0.2290169215622046, 0.3488570678559591, 0.43314325779159024, 0.688108068206336, 0.5162219660781141, 0.492886773158868, 0.5789725788056281, 0.41542039302633044, 0.4333205318347124, 0.7386770411565124, 0.6682089995056355, 0.7834550794408761, 0.4077117867314423, 0.40054031762880854, 0.5158807056252025, 0.5012894239930991, 0.7787616213839844, 0.8303338976572383, 0.5625037952930176, 0.4815594156028131, 0.8184831772015866, 0.5918058853493386, 0.33574467934540764, 0.5321023068926671, 0.7367642275105969, 0.5996544556805294, 0.5190939189419399, 0.375012127751613, 0.546609646502366, 0.6514721740507518, 0.20947167839482378, 0.5748519943482926, 0.47271810579379114, 0.6587501465625146, 0.4080610320912135, 0.685092597348709, 0.8912058800007714, 0.4072595916008566, 0.6000378991970468, 0.49909749789692615, 0.3252646046441334, 0.2174757435815441, 0.3549735292792559, 0.6579204169377896, 0.5612452354815397, 0.5333106010325431, 0.6244401183459501, 0.5064261123055607, 0.5945502976781354, 0.5307575158856397, 0.46870597412474824, 0.8374437767641855, 0.8927474513354525, 0.26154772389361136, 0.8519802400363303, 0.7506644877281048, 0.43219874175555695, 0.7691182574071866, 0.5706658931758056, 0.6319855345731491, 0.6183616274163133, 0.51331138682916, 0.6790387022019999, 0.4826425204639738, 0.7521965084436367, 0.6514010799864784, 0.9292572905772767, 0.869647352714207, 0.7240566707458733, 0.2884457814062874, 0.7832799072597294, 0.22922522466780756, 0.6872672878334557, 0.365460952037475, 0.5068836983780596, 0.8104598991023152, 0.26856474159172017, 0.4187223657952392, 0.4527908751937714, 0.8243423535224534, 0.8240574882447643, 0.5791905950052828, 0.7542419525495427, 0.43259418226821933, 0.4107390621941692, 0.7921768924863931, 0.7952528745642176, 0.500122209133867, 0.7134443834515587, 0.5768035432322688, 0.5649712442330139, 0.5431726181266734, 0.7466812379110405, 0.6482879174875628, 0.12884134200148475, 0.43569738318018275, 0.7192945540238429, 0.8450551919993065, 0.36141366431902805, 0.5707663277286219, 0.4843444811091596, 0.6114242000826401, 0.5307410039792388, 0.366371341994678, 0.19761930022726484, 0.41833447859197814, 0.5353821299611372, 0.5689000584617496, 0.6922623212752947, 0.5477557591525017, 0.48391705456950146, 0.8942151770821865, 0.840904453126078, 0.5394284709798496, 0.6071438826345581, 0.5408796510863387, 0.5383423878182904, 0.4374077636741214, 0.3267127975955766, 0.7467210006638636, 0.3806510829189784, 0.1820446901028259, 0.7734300417855348, 0.30666098377498574, 0.3965389900248738, 0.6490121796997295, 0.45839841082325505, 0.7238107839143577, 0.5729215536589831, 0.6340832510401675, 0.2616212453210304, 0.7582229822027793, 0.5442851395227176, 0.30041226213662975, 0.4194717322205429, 0.4989260928892314, 0.6253459517113702, 0.4420744998196169, 0.572806219372709, 0.6178154066147622, 0.28939945709819465, 0.5438928165036324, 0.4940370284051592, 0.4312329092702174, 0.8101489130699882, 0.6538750313981949, 0.5207379412424551, 0.5668745180838458, 0.8718277798045143, 0.5385322250463479, 0.2608194756144132, 0.5516318264956713, 0.542671968372451, 0.22492888047384113, 0.6337140511938253, 0.8036418104561536, 0.8781287385614391, 0.5659436309624821, 0.4586356523696369, 0.574072214608327, 0.2511362844514941, 0.3318474542224905, 0.28333368107940005, 0.49023117333949046, 0.7385284269940436, 0.7814566588717384, 0.7427841550701308, 0.23127375099467695, 0.6760320887507192, 0.6608075996583815, 0.7376354654821852, 0.4787042712800358, 0.650028858145369, 0.3915547441952573, 0.44343629254699934, 0.6220565366250902, 0.3567913963466017, 0.3267455220139449, 0.7268913379859014, 0.404872192380924, 0.5278779376090724, 0.6949804706617532, 0.8533990305468535, 0.39780010371772445, 0.3559385799939352, 0.9275691280323417, 0.6053819647285794, 0.490349983430302, 0.6934132918324156, 0.7380696030883859, 0.6628289990060665, 0.7357699338455306, 0.16949118188433127, 0.6629723496613199, 0.4821961600878496, 0.7236834358077405, 0.558556159325682, 0.21896684193061489, 0.6525942408124537, 0.5373729666623905, 0.8408291047782359, 0.5763591743485517, 0.35909201933863577, 0.7692117601307078, 0.865931895039148, 0.21060624076396123, 0.6845621056012153, 0.1935111910915459, 0.6271005382554999, 0.6010900713789156, 0.65839268020116, 0.670455229935114, 0.7134983195245689, 0.35227350516355405, 0.4522962373672012, 0.430019266016445, 0.4930233449386294, 0.3082964613320997, 0.8200915642752693, 0.5313393371293147, 0.7695928982950905, 0.8681547639853407, 0.6445921681885581, 0.5040756098699638, 0.574319545823131, 0.3027187142262456, 0.3648123456075387, 0.492373712320206, 0.8139917067487897, 0.772672554848935, 0.5412336365816965, 0.5502285058577748, 0.8162711049772523, 0.762032647154943, 0.7843985843830485, 0.9308408744278986, 0.568935295203631, 0.7323318381033639, 0.6965715268271967, 0.5590377275864223, 0.6900988194223485, 0.41710262177461827, 0.6465137553195115, 0.7653186752219449, 0.3833674002054715, 0.7727529849573804, 0.7499044856090894, 0.6086438460170094, 0.42803581656172596, 0.4828545879925603, 0.7516792643928107, 0.7462704075089216, 0.21575352883725968, 0.37399478979928763, 0.23967262013117524, 0.8204088607378107, 0.6539227113244089, 0.566534198688098, 0.6911568151579116, 0.32930654567979456, 0.3639204568355233, 0.8140661964674378, 0.2640374027740079, 0.19669452566566495, 0.6593768947083595, 0.6682925549800763, 0.6110937980855516, 0.506934407731548, 0.6189674666752771, 0.3168243474559227, 0.39148341542425735, 0.7112552064784551, 0.8784033137337742, 0.8813259663450006, 0.3930632912082492, 0.6320588606417955, 0.7204297785032558, 0.3798181481501237, 0.6314856721416581, 0.7176925968614573, 0.5362995126631233, 0.4860356733835421, 0.1455575615260033, 0.27264447755339305, 0.2786753934740092, 0.4095579926640084, 0.570027547123419, 0.8884299749428806, 0.4106704766910993, 0.7118349666150712, 0.4897542547862491, 0.3499659626106872, 0.8695656557325835, 0.592937095734148, 0.703553693284689, 0.9072841696537222, 0.47021324486449784, 0.222131042121849, 0.7676404974537385, 0.5802432450675726, 0.6644965278046866, 0.5566934610512383, 0.9282151563744275, 0.4398710537781339, 0.4903107118140383, 0.5649676058816154, 0.4943930820605468, 0.591575140680095, 0.914050379577698, 0.3732237133613603, 0.33822831255828584, 0.21357510982761516, 0.6152230293747528, 0.5004344506548116, 0.3606783082639838, 0.31389528954500684, 0.5128556270766564, 0.9353061866122727, 0.7112595251402384, 0.26217900350880174, 0.5933964649985269, 0.773016471840963, 0.6816607883642304, 0.8113063738782503, 0.2626948463538771, 0.6580154718933391, 0.9029004575468477, 0.5834224188070313, 0.8653296327337283, 0.4079915145554705, 0.5518584864427217, 0.7637522430375667, 0.4778363034882701, 0.5295121138935452, 0.5052678870046177, 0.28750332033233067, 0.37028671281034564, 0.5010977874135292, 0.5707248171051735, 0.9109691036171016, 0.23227600850216717, 0.7765813374498813, 0.8686260807431587, 0.4108844612418484, 0.6168378097688202, 0.5698969573197332, 0.8070715404506752, 0.6274819282904239, 0.7366303421219791, 0.41471607291674023, 0.6688431532673562, 0.3104848364374154, 0.3458549829737922, 0.44156594564679646, 0.47085716231552577, 0.5677592752620001, 0.6687850614709024, 0.6115007905704588, 0.2282602462303565, 0.50275142195644, 0.4857942117939656, 0.5246421106356757, 0.6872006575232905, 0.8069816442449187, 0.2223826381907207, 0.6822160103691088, 0.33902047398354335, 0.6024270133360492, 0.3884878693669341, 0.5841861236793902, 0.5415549218212172, 0.5541300303636029, 0.6023811328062602, 0.34108815697107275, 0.2912456792972941, 0.5189765540246923, 0.7887884274027104, 0.8046838325956206, 0.44782361386828307, 0.6871536927414691, 0.6136320216295639, 0.6884203233138786, 0.14793505003564617, 0.6272240548822717, 0.28816414786634437, 0.6206896342583065, 0.25069653166306805, 0.6077296661803484, 0.7573641875620682, 0.5001466296767978, 0.7401960770276752, 0.5082313511236376, 0.45072530729680355, 0.6451076643267338, 0.6364257824831183, 0.63078826312171, 0.7455055647329015, 0.6774579422368767, 0.24450424928364645, 0.6207206259784981, 0.6067392749183433, 0.3296259760687481, 0.5692227157575981, 0.5742197031870906, 0.675765367059187, 0.8015124116980378, 0.5826595610265404, 0.5353618935925422, 0.6984953367295221, 0.6935507157544542, 0.3599833979524768, 0.446657151493686, 0.6807795191676316, 0.2688408008168709, 0.6884986134921022, 0.5148429352352875, 0.839635459263536, 0.6571418566131596, 0.4746895434441805, 0.5012906449261054, 0.40924090344113473, 0.7984424935610968, 0.6179878168207047, 0.5399565567494862, 0.5495630984287695, 0.6825759442383689, 0.36828314174479493, 0.8063596534134853, 0.49031586919146425, 0.7490619415758346, 0.6076610919185718, 0.5103076865561114, 0.5183933109744313, 0.8921339618582602, 0.3611222521571872, 0.8230340032276884, 0.6315424908326062, 0.7535152651277437, 0.6904308677273676, 0.6739132481811262, 0.42623810555419805, 0.5910356719482845, 0.7434329704923217, 0.5938258317593229, 0.6549717042145111, 0.23153545374116202, 0.2835434695863417, 0.5880541665850241, 0.21515396172339274, 0.4778746684652747, 0.6622401653670479, 0.6584478135483849, 0.6470955359768451]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#   space_10k cosine scores:\n",
    "word_pairs_arrays = []\n",
    "for w1, w2 in word_pairs:\n",
    "    if w1 in space_10k and w2 in space_10k:\n",
    "       word_tuple = [space_10k[w1]],[space_10k[w2]]\n",
    "    word_pairs_arrays.append(word_tuple)\n",
    "raw_similarities = []\n",
    "for w1, w2 in word_pairs_arrays:\n",
    "    raw_similarities.append(cosine_similarity(w1,w2))\n",
    "scores_space10k = []\n",
    "for item in raw_similarities:\n",
    "    scores_space10k.append(item[0][0]) \n",
    "print(f\"space_10k cosine scores {scores_space10k}\")\n",
    "\n",
    "#   ppmispace cosine scores:\n",
    "word_pairs_arrays = []\n",
    "for w1, w2 in word_pairs:\n",
    "    if w1 in ppmispace_10k and w2 in ppmispace_10k:\n",
    "       word_tuple = [ppmispace_10k[w1]],[ppmispace_10k[w2]]\n",
    "    word_pairs_arrays.append(word_tuple)\n",
    "ppmi_similarities = []\n",
    "for w1, w2 in word_pairs_arrays:\n",
    "    ppmi_similarities.append(cosine_similarity(w1,w2))\n",
    "scores_ppmi = []\n",
    "for item in ppmi_similarities:\n",
    "    scores_ppmi.append(item[0][0]) \n",
    "print(f\"ppmispace_10k cosine scores {scores_ppmi}\")\n",
    "\n",
    "#   svd space cosine scores:\n",
    "word_pairs_arrays = []\n",
    "for w1, w2 in word_pairs:\n",
    "    if w1 in svdspace_10k and w2 in svdspace_10k:\n",
    "       word_tuple = [svdspace_10k[w1]],[svdspace_10k[w2]]\n",
    "    word_pairs_arrays.append(word_tuple)\n",
    "svd_similarities  = []\n",
    "for w1, w2 in word_pairs_arrays:\n",
    "    svd_similarities.append(cosine_similarity(w1,w2))\n",
    "scores_svd = []\n",
    "for item in svd_similarities:\n",
    "    scores_svd.append(item[0][0]) \n",
    "print(f\"svdspace_10k cosine scores {scores_svd}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate correlation coefficients between lists of similarity scores and the real semantic similarity scores from the experiment. The scores of what model best correlates them? Is this expected? **[6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity vs. Cos sim scores (space10k):\n",
      "rho     = 0.1522\n",
      "p-value = 0.0000\n",
      "Semantic Similarity vs. Cos sim scores (PPMI):\n",
      "rho     = 0.4547\n",
      "p-value = 0.0000\n",
      "Semantic Similarity vs. Cos sim scores (SVD):\n",
      "rho     = 0.4232\n",
      "p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# space10k model vs. semantic similarity scores\n",
    "rho1, pval1 = stats.spearmanr(semantic_similarity, scores_space10k)\n",
    "print(\"\"\"Semantic Similarity vs. Cos sim scores (space10k):\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho1, pval1)) \n",
    "\n",
    "# ppmi model vs. semantic similarity scores\n",
    "rho2, pval2 = stats.spearmanr(semantic_similarity, scores_ppmi)\n",
    "print(\"\"\"Semantic Similarity vs. Cos sim scores (PPMI):\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho2, pval2)) \n",
    "\n",
    "# svd model vs. semantic similarity scores\n",
    "rho3, pval3 = stats.spearmanr(semantic_similarity, scores_svd)\n",
    "print(\"\"\"Semantic Similarity vs. Cos sim scores (SVD):\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho3, pval3)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the PPMI model's scores have the highest correlation with the semantic similarity scores from the experiment,with a correlation coefficient (rho) of 0.4547. PPMI is a good measure to use when looking into words co-occuring, which is in a way what we are doing here (i.e. \"repair\" - \"car\", \"car\" - \"shop\", \"shop\" - \"bag\" could all co-occur and also share semantic similarity). The SVD model also shows a significant correlation but slightly lower than PPMI (rho = 0.4232). We think the SVD space might be even too dense, causing some of the meanings to disappear and thus the correlation score is not very high. On the other hand, the space10k model,shows a considerably lower correlation (rho = 0.1522). Which suggests that simpler count-based methods may not adequately reflect the complexities of semantic similarity as they lack the depth in modeling complex semantic or contextual relationships compared to probability-based models like PPMI.\n",
    "Overall,not only does the PPMI model perform better than the other two models, but it also meets expectations concerning the efficacy of models that use statistical probabilities to simulate human-like word semantic comprehension. This shows the significance of usingadvanced models for semantic analysis, such as PPMI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate correlation coefficients between lists of cosine similarity scores and the real visual similarity scores from the experiment. Which similarity model best correlates with them? How do the correlation coefficients compare with those from the previous comparison - and can you speculate why do we get such results? **[7 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual Similarity vs. Cos sim scores (space10k):\n",
      "rho     = 0.1212\n",
      "p-value = 0.0007\n",
      "Visual Similarity vs. Cos sim scores (PPMI):\n",
      "rho     = 0.3838\n",
      "p-value = 0.0000\n",
      "Visual Similarity vs. Cos sim scores (SVD):\n",
      "rho     = 0.3097\n",
      "p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# space10k model vs. visual similarity scores\n",
    "rho4, pval4 = stats.spearmanr(visual_similarity, scores_space10k)\n",
    "print(\"\"\"Visual Similarity vs. Cos sim scores (space10k):\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho4, pval4)) \n",
    "\n",
    "# ppmi model vs. visual similarity scores\n",
    "rho5, pval5 = stats.spearmanr(visual_similarity, scores_ppmi)\n",
    "print(\"\"\"Visual Similarity vs. Cos sim scores (PPMI):\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho5, pval5)) \n",
    "\n",
    "# svd model vs. visual similarity scores\n",
    "rho6, pval6 = stats.spearmanr(visual_similarity, scores_svd)\n",
    "print(\"\"\"Visual Similarity vs. Cos sim scores (SVD):\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho6, pval6)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The space10k model shows the lowest correlation with visual similarity scores (rho = 0.1212),which indicates a weak association.\n",
    "The SVD models shows a moderate correlation (rho = 0.3097), a bit better than space10k but still lower than PPMI.\n",
    "The PPMI model shows the highest correlation (rho = 0.3838), which suggests a strong relationship between the PPMI-derived cosine similarities and visual similarity scores.\n",
    "In comparison to semantic similarity, visual similarity correlation coefficients are frequently lower across all models. For example, PPMI linked with semantic similarity at 0.4547, but not with visual similarity at 0.3838.\n",
    "\n",
    "(note: 0.4547 is the Spearman's rho value indicating how well the PPMI model correlates with semantic similarity scores.\n",
    "0.3838 is the Spearman's rho value for how well the PPMI model correlates with visual similarity scores.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Operations on similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform mathematical operations on vectors to derive meaning predictions.\n",
    "\n",
    "For example, we can perform `king - man` and add the resulting vector to `woman` and we hope to get the vector for `queen`. What would be the result of `stockholm - sweden + denmark`? Why? **[3 marks]**\n",
    "\n",
    "If you want to learn more about vector differences between words (and words in analogy relations), check this paper [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result would be Copenhagen because if you deduct Stockholm from Sweden you get the vector for capital city and if you add Denmark to that vector you get the vector for the capital of Denmark.\n",
    "We are thus looking for the Danish version of the vector for Stockholm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code that allows us to calculate such comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def normalize(vec):\n",
    "    return vec / veclen(vec)\n",
    "\n",
    "def find_similar_to(vec1, space):\n",
    "    # vector similarity funciton\n",
    "    #sim_fn = lambda a, b: 1-distance.euclidean(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.correlation(a, b)\n",
    "    #sim_fn = lambda a, b: 1-distance.cityblock(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.chebyshev(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: np.dot(normalize(a), normalize(b))\n",
    "    sim_fn = lambda a, b: 1-distance.cosine(a, b)\n",
    "\n",
    "    sims = [\n",
    "        (word2, sim_fn(vec1, space[word2]))\n",
    "        for word2 in space.keys()\n",
    "    ]\n",
    "    return sorted(sims, key = lambda p:p[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you apply this code. Comment on the results you get. **[3 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('long', 0.8733111261346901),\n",
       " ('above', 0.8259671977311955),\n",
       " ('around', 0.8030776291120685),\n",
       " ('sun', 0.7692439111243973),\n",
       " ('just', 0.7678481974778111),\n",
       " ('wide', 0.767257431992253),\n",
       " ('each', 0.7665960260861158),\n",
       " ('circle', 0.7647746702909336),\n",
       " ('length', 0.7601066921319761),\n",
       " ('almost', 0.7542351860536628)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short = normalize(svdspace_10k['short'])\n",
    "light = normalize(svdspace_10k['light'])\n",
    "long = normalize(svdspace_10k['long'])\n",
    "heavy = normalize(svdspace_10k['heavy'])\n",
    "\n",
    "\n",
    "find_similar_to(light - (heavy - long), svdspace_10k)[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expected to receive a vector corresponding to \"short\", as it is to light what long is to heavy. At least the words \"above\", \"around\", \"length\", and \"wide\" could be seen as semantically somewhat similar to \"short\". We found it a bit suprising that the most similar word on the list was \"long\". After some discussions we came to the conclusion that these particular word embeddings have some limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find 5 similar pairs of pairs of words and test them. Hint: google for `word analogies examples`. You can also construct analogies that are not only lexical but also express other relations such as grammatical relations, e.g. `see, saw, leave, ?` or analogies that are based on world knowledge as in `question-words.txt` from the [Google analogy dataset](http://download.tensorflow.org/data/questions-words.txt) described in [3]. Does the resulting vector similarity confirm your expectations? Remember you can only do this test if the words are contained in our vector space with 10,000 dimensions. **[10 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lock', 0.8590408311319814),\n",
       " ('pull', 0.7624097320665246),\n",
       " ('switch', 0.753712116048725),\n",
       " ('sink', 0.7478139934038065),\n",
       " ('throw', 0.7448945150317656),\n",
       " ('shut', 0.7389661604600366),\n",
       " ('burn', 0.7338682111029614),\n",
       " ('hook', 0.7204634584479472),\n",
       " ('shoot', 0.7197754429847165),\n",
       " ('stick', 0.7138448857939145)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queen = normalize(svdspace_10k['queen'])\n",
    "king = normalize(svdspace_10k['king'])\n",
    "man = normalize(svdspace_10k['man'])\n",
    "woman = normalize(svdspace_10k['woman'])\n",
    "sun = normalize(svdspace_10k['sun'])\n",
    "day = normalize(svdspace_10k['day'])\n",
    "moon = normalize(svdspace_10k['moon'])\n",
    "night = normalize(svdspace_10k['night'])\n",
    "book = normalize(svdspace_10k['book'])\n",
    "library= normalize(svdspace_10k['library'])\n",
    "song= normalize(svdspace_10k['song'])\n",
    "concert = normalize(svdspace_10k['concert'])\n",
    "bird = normalize(svdspace_10k['bird'])\n",
    "nest = normalize(svdspace_10k['nest'])\n",
    "spider = normalize(svdspace_10k['spider'])\n",
    "web = normalize(svdspace_10k['web'])\n",
    "key = normalize(svdspace_10k['key'])\n",
    "lock = normalize(svdspace_10k['lock'])\n",
    "switch = normalize(svdspace_10k['switch'])\n",
    "light = normalize(svdspace_10k['light'])\n",
    "\n",
    "\n",
    "\n",
    "find_similar_to(woman - (king - man), svdspace_10k)[:10]\n",
    "find_similar_to( moon - (sun - day),svdspace_10k)[:10]\n",
    "find_similar_to(song - (book - library) ,svdspace_10k)[:10]\n",
    "find_similar_to(spider - (bird - nest) ,svdspace_10k)[:10]\n",
    "find_similar_to(switch - (key - lock) ,svdspace_10k)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We couldn't manage to print all the results together. However, when looking at them individually the results seem relevant but it rarely gets the vector we expected. It only works for the sun is to day what moon is to night. In the rest of the analogies it just gives some similar words but not the one we are looking for. Perhaps this has to do with the complexity of the vectors we are using, the information they contain isn't enough to deal with all these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic composition and phrase similarity **[20 marks]**\n",
    "\n",
    "In this task, we are going to examine at how the composed vectors of phrases by different semantic composition functions/models introduced in [2] correlate with human judgements of similarity between phrases. We will use the the dataset from this paper which is stored in `mitchell_lapata_acl08.txt`. If you are interested about furtehr details about this task also refer to this paper.\n",
    "\n",
    "(i) Process the dataset. The dataset contains human judgemements of similarity between phrases recorded one per line. The first column indicates the id of a participant making a judgement (`participant`), the next column is `verb`, followed by `noun` and `landmark`. From these three columns we can construct phrases that were compared by human informants, namely `verb noun` vs `verb landmark`. The next column `input` indicates a similarity score a participant assigned to a pair of such phrases on a scale from 1 to 7 where 1 is lowest and 7 is highest. The last column `hilo` groups the phrases into two sets: phrases where we expect low and phrases where we expect high similarity scores. This is because we want to test our compositional functions on two tasks and examine whether a function is discriminative between them. Correlation between scores could also be due to other reasons than semantic similarity and hence good prediction on both tasks simultaneously shows that a function is truly discriminating the phrases using some semantic criteria.\n",
    "\n",
    "For extracting information you can use the code from the lecture to start with. How to structure this data is up to you - a dictionary-like format would be a good choice. Remember that each example was judged by several participants and phrases will repeat in the dataset. Therefore, you have to collect all judgments for a particular set of phrases and average them. This will become useful in step (iii).\n",
    "\n",
    "(ii) Compose the vectors of the extracted word pairs by testing different compositional functions. In the lecture we introduced simple additive, simple multiplicative and combined models (details are described in [2]). Your task is to take a pair of phrases, e.g. the first example in the dataset `stray thought` and `stray roam` and for each phrase compute a composition of the vectors of their words using these functions, using one function per experiment run. For each phrase you will get a single vector. You can encode the words with any vector space introduced earlier (standard space, ppmi or svd) but your code should be structured in a way that it will be easy to switch between them. Finally, take the resulting (composed) vectors of phrase pairs in the dataset and calculate a cosine similarity between them.\n",
    "\n",
    "(iii) Now you have cosine similairity scores between vectors of phrases but how do they compare with the average human scores that you calculated from the individual judgements from the `input` column of the dataset for the same phrases? Calculate Spearman rank correlation coefficient between two lists of the scores both for the `high` and the `low` task . \n",
    "\n",
    "We use the Spearmank rank correlation coefficient (or Spearman's rho) rather than Peason's correlation coefficent because we cannot compare cosine scores with human judgements directly. Cosine is a constinuous measure and human judgements are expressed as ranks. Also, we cannot say if 0.28 to 1 is the same (or different) to 6 to 7 in the human scores.  The Spearman rank correlation coeffcient turns the scores for all examples within each group first to ranks and then these ranks are correlated (or approximated to a linear function). \n",
    "\n",
    "In the end you should get a table similar to the one below from the paper. What is the best compositional function from those that you evaluated with your vector spaces and why?\n",
    "\n",
    "<img src=\"res.png\" alt=\"drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(('stray', 'thought'), ('stray', 'roam'), 'low'): [5.294117647058823], (('stray', 'discussion'), ('stray', 'digress'), 'high'): [5.529411764705882], (('stray', 'eye'), ('stray', 'roam'), 'high'): [5.176470588235294], (('stray', 'child'), ('stray', 'digress'), 'low'): [3.0], (('throb', 'body'), ('throb', 'pulse'), 'high'): [5.029411764705882], (('throb', 'head'), ('throb', 'shudder'), 'low'): [2.3529411764705883], (('throb', 'voice'), ('throb', 'shudder'), 'low'): [2.8823529411764706], (('throb', 'vein'), ('throb', 'pulse'), 'high'): [6.205882352941177], (('chatter', 'machine'), ('chatter', 'click'), 'high'): [4.323529411764706], (('chatter', 'girl'), ('chatter', 'gabble'), 'high'): [4.764705882352941], (('chatter', 'child'), ('chatter', 'click'), 'low'): [2.176470588235294], (('chatter', 'tooth'), ('chatter', 'gabble'), 'low'): [2.176470588235294], (('rebound', 'shot'), ('rebound', 'ricochet'), 'high'): [5.735294117647059], (('rebound', 'ball'), ('rebound', 'rally'), 'low'): [2.5], (('rebound', 'share'), ('rebound', 'ricochet'), 'low'): [3.5], (('rebound', 'optimism'), ('rebound', 'rally'), 'high'): [3.911764705882353], (('flicker', 'tongue'), ('flicker', 'waver'), 'low'): [2.7941176470588234], (('flicker', 'screen'), ('flicker', 'flick'), 'high'): [3.323529411764706], (('flicker', 'interest'), ('flicker', 'waver'), 'high'): [4.264705882352941], (('flicker', 'hope'), ('flicker', 'flick'), 'low'): [2.088235294117647], (('subside', 'flood'), ('subside', 'lessen'), 'high'): [5.735294117647059], (('subside', 'fear'), ('subside', 'lessen'), 'high'): [5.9411764705882355], (('subside', 'symptom'), ('subside', 'sink'), 'low'): [2.588235294117647], (('subside', 'island'), ('subside', 'sink'), 'high'): [4.0588235294117645], (('slump', 'shoulder'), ('slump', 'slouch'), 'high'): [5.911764705882353], (('slump', 'sale'), ('slump', 'slouch'), 'low'): [4.117647058823529], (('slump', 'value'), ('slump', 'decline'), 'high'): [5.647058823529412], (('slump', 'man'), ('slump', 'decline'), 'low'): [1.8823529411764706], (('bow', 'butler'), ('bow', 'submit'), 'low'): [2.9411764705882355], (('bow', 'company'), ('bow', 'submit'), 'high'): [4.323529411764706], (('bow', 'head'), ('bow', 'stoop'), 'high'): [4.470588235294118], (('bow', 'government'), ('bow', 'stoop'), 'low'): [3.1470588235294117], (('erupt', 'fountain'), ('erupt', 'burst'), 'high'): [3.4411764705882355], (('erupt', 'temper'), ('erupt', 'flare'), 'high'): [5.352941176470588], (('erupt', 'storm'), ('erupt', 'flare'), 'low'): [4.176470588235294], (('erupt', 'conflict'), ('erupt', 'burst'), 'low'): [3.7941176470588234], (('recoil', 'heart'), ('recoil', 'flinch'), 'high'): [3.5], (('recoil', 'rifle'), ('recoil', 'kick'), 'high'): [5.205882352941177], (('recoil', 'eye'), ('recoil', 'flinch'), 'high'): [3.6176470588235294], (('recoil', 'hand'), ('recoil', 'kick'), 'low'): [1.911764705882353], (('boom', 'noise'), ('boom', 'prosper'), 'low'): [2.1176470588235294], (('boom', 'export'), ('boom', 'prosper'), 'high'): [5.529411764705882], (('boom', 'sale'), ('boom', 'thunder'), 'low'): [2.7941176470588234], (('boom', 'gun'), ('boom', 'thunder'), 'high'): [5.617647058823529], (('waver', 'opinion'), ('waver', 'fluctuate'), 'high'): [5.0], (('waver', 'concentration'), ('waver', 'falter'), 'low'): [5.705882352941177], (('waver', 'courage'), ('waver', 'fluctuate'), 'low'): [3.8823529411764706], (('waver', 'determination'), ('waver', 'falter'), 'high'): [5.5], (('flare', 'cigarette'), ('flare', 'erupt'), 'low'): [2.6470588235294117], (('flare', 'eye'), ('flare', 'flame'), 'high'): [4.5], (('flare', 'argument'), ('flare', 'erupt'), 'high'): [5.205882352941177], (('flare', 'row'), ('flare', 'flame'), 'low'): [3.676470588235294], (('reel', 'industry'), ('reel', 'whirl'), 'low'): [3.088235294117647], (('reel', 'mind'), ('reel', 'whirl'), 'high'): [4.705882352941177], (('reel', 'head'), ('reel', 'stagger'), 'low'): [2.411764705882353], (('reel', 'man'), ('reel', 'stagger'), 'high'): [4.5], (('glow', 'fire'), ('glow', 'burn'), 'high'): [4.323529411764706], (('glow', 'face'), ('glow', 'beam'), 'high'): [5.411764705882353], (('glow', 'cigar'), ('glow', 'beam'), 'low'): [2.3823529411764706], (('glow', 'skin'), ('glow', 'burn'), 'low'): [2.411764705882353], (('stray', 'thought'), ('stray', 'digress'), 'high'): [6.038461538461538], (('stray', 'discussion'), ('stray', 'roam'), 'low'): [4.846153846153846], (('stray', 'eye'), ('stray', 'digress'), 'low'): [3.3461538461538463], (('stray', 'child'), ('stray', 'roam'), 'high'): [4.6923076923076925], (('throb', 'body'), ('throb', 'shudder'), 'low'): [3.576923076923077], (('throb', 'head'), ('throb', 'pulse'), 'high'): [5.538461538461538], (('throb', 'voice'), ('throb', 'pulse'), 'high'): [4.384615384615385], (('throb', 'vein'), ('throb', 'shudder'), 'low'): [2.8076923076923075], (('flare', 'cigarette'), ('flare', 'flame'), 'high'): [5.038461538461538], (('flare', 'eye'), ('flare', 'erupt'), 'low'): [3.269230769230769], (('flare', 'argument'), ('flare', 'flame'), 'low'): [4.5], (('flare', 'row'), ('flare', 'erupt'), 'high'): [5.8076923076923075], (('rebound', 'share'), ('rebound', 'rally'), 'high'): [5.076923076923077], (('rebound', 'ball'), ('rebound', 'ricochet'), 'high'): [6.153846153846154], (('rebound', 'shot'), ('rebound', 'rally'), 'low'): [2.576923076923077], (('rebound', 'optimism'), ('rebound', 'ricochet'), 'low'): [3.0384615384615383], (('flicker', 'screen'), ('flicker', 'waver'), 'low'): [4.115384615384615], (('flicker', 'interest'), ('flicker', 'flick'), 'low'): [2.576923076923077], (('flicker', 'tongue'), ('flicker', 'flick'), 'high'): [3.730769230769231], (('flicker', 'hope'), ('flicker', 'waver'), 'high'): [4.6923076923076925], (('slump', 'shoulder'), ('slump', 'decline'), 'low'): [3.423076923076923], (('slump', 'sale'), ('slump', 'decline'), 'high'): [6.269230769230769], (('slump', 'value'), ('slump', 'slouch'), 'low'): [3.5], (('slump', 'man'), ('slump', 'slouch'), 'high'): [5.923076923076923], (('bow', 'butler'), ('bow', 'stoop'), 'high'): [4.115384615384615], (('bow', 'company'), ('bow', 'stoop'), 'low'): [2.923076923076923], (('bow', 'head'), ('bow', 'submit'), 'low'): [3.269230769230769], (('bow', 'government'), ('bow', 'submit'), 'high'): [5.384615384615385], (('erupt', 'storm'), ('erupt', 'burst'), 'high'): [4.8076923076923075], (('erupt', 'fountain'), ('erupt', 'flare'), 'low'): [3.9615384615384617], (('erupt', 'temper'), ('erupt', 'burst'), 'low'): [5.038461538461538], (('erupt', 'conflict'), ('erupt', 'flare'), 'high'): [6.038461538461538], (('recoil', 'heart'), ('recoil', 'kick'), 'low'): [2.4615384615384617], (('recoil', 'rifle'), ('recoil', 'flinch'), 'low'): [2.8846153846153846], (('recoil', 'eye'), ('recoil', 'kick'), 'low'): [2.0384615384615383], (('recoil', 'hand'), ('recoil', 'flinch'), 'high'): [5.153846153846154], (('boom', 'noise'), ('boom', 'thunder'), 'high'): [6.115384615384615], (('boom', 'export'), ('boom', 'thunder'), 'low'): [2.769230769230769], (('boom', 'gun'), ('boom', 'prosper'), 'low'): [1.9615384615384615], (('boom', 'sale'), ('boom', 'prosper'), 'high'): [6.5], (('waver', 'opinion'), ('waver', 'falter'), 'low'): [5.384615384615385], (('waver', 'concentration'), ('waver', 'fluctuate'), 'high'): [5.076923076923077], (('waver', 'courage'), ('waver', 'falter'), 'high'): [6.269230769230769], (('waver', 'determination'), ('waver', 'fluctuate'), 'low'): [5.461538461538462], (('subside', 'flood'), ('subside', 'sink'), 'low'): [3.8846153846153846], (('subside', 'fear'), ('subside', 'sink'), 'low'): [4.538461538461538], (('subside', 'symptom'), ('subside', 'lessen'), 'high'): [6.538461538461538], (('subside', 'island'), ('subside', 'lessen'), 'low'): [2.8076923076923075], (('chatter', 'machine'), ('chatter', 'gabble'), 'low'): [4.1923076923076925], (('chatter', 'girl'), ('chatter', 'click'), 'low'): [1.9615384615384615], (('chatter', 'tooth'), ('chatter', 'click'), 'high'): [4.0], (('chatter', 'child'), ('chatter', 'gabble'), 'high'): [5.423076923076923], (('reel', 'industry'), ('reel', 'stagger'), 'high'): [4.8076923076923075], (('reel', 'mind'), ('reel', 'stagger'), 'low'): [4.076923076923077], (('reel', 'head'), ('reel', 'whirl'), 'high'): [5.3076923076923075], (('reel', 'man'), ('reel', 'whirl'), 'low'): [4.153846153846154], (('glow', 'fire'), ('glow', 'beam'), 'low'): [3.423076923076923], (('glow', 'skin'), ('glow', 'beam'), 'high'): [5.0], (('glow', 'cigar'), ('glow', 'burn'), 'high'): [5.423076923076923], (('glow', 'face'), ('glow', 'burn'), 'low'): [3.9615384615384617]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"./mitchell_lapata_acl08.txt\") as f:\n",
    "    text = f.readlines()\n",
    "\n",
    "averages_dict = {}\n",
    "\n",
    "for line in text[1:]:\n",
    "    line = line.split()\n",
    "    values_key = ((line[1], line[2]), (line[1], line[3]),line[5])\n",
    "    value = int(line[4])\n",
    "    count = 1\n",
    "    if values_key in averages_dict:\n",
    "        averages_dict[values_key][0] += value\n",
    "        averages_dict[values_key][1] += 1\n",
    "    else:\n",
    "        averages_dict[values_key] = [value, count]\n",
    "#print(averages_dict)\n",
    "for key in averages_dict:\n",
    "    sum_value = averages_dict[key][0]\n",
    "    count = averages_dict[key][1]\n",
    "    average = sum_value / count\n",
    "    averages_dict[key] = [average]\n",
    "\n",
    "print(averages_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(('bow', 'butler'), ('bow', 'submit'), 'low'): array([[0.73848528]]), (('bow', 'company'), ('bow', 'submit'), 'high'): array([[0.81739913]]), (('boom', 'sale'), ('boom', 'thunder'), 'low'): array([[0.902861]]), (('boom', 'gun'), ('boom', 'thunder'), 'high'): array([[0.95037681]]), (('bow', 'head'), ('bow', 'submit'), 'low'): array([[0.80201642]]), (('bow', 'government'), ('bow', 'submit'), 'high'): array([[0.81736778]]), (('boom', 'noise'), ('boom', 'thunder'), 'high'): array([[0.96713142]]), (('boom', 'export'), ('boom', 'thunder'), 'low'): array([[0.95332669]])}\n"
     ]
    }
   ],
   "source": [
    "cosine_similarities = {}\n",
    "for key,value in averages_dict.items():\n",
    "    #print(key[1])\n",
    "    try:\n",
    "        #print(key)\n",
    "        #print(key[0][0])\n",
    "        vector1 = normalize(space_10k[key[0][0]]) \n",
    "        \n",
    "        #print(vector1)\n",
    "        #print(key[0][1])\n",
    "        vector2 = normalize(space_10k[key[0][1]])\n",
    "        #print(vector2)\n",
    "        #print(key[1][1])\n",
    "        vector3 = normalize(space_10k[key[1][1]])\n",
    "        #print(vector3)\n",
    "        if len(vector1) > 0 and len(vector2) > 0 and len(vector3) >0:\n",
    "            vectors1=[vector1+vector2]\n",
    "        #print(vectors1)\n",
    "            vectors2= [vector1+vector3]\n",
    "        #print (f' Vector 2 {vectors2}')\n",
    "            #cosine_similarities.append(cosine_similarity(vectors1,vectors2))\n",
    "            cosine_similarities[key] = cosine_similarity(vectors1,vectors2)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination = {}\n",
    "for key,value in averages_dict.items():\n",
    "    for key_cos,value_cos in cosine_similarities.items():\n",
    "        average_value = averages_dict[key_cos]\n",
    "        combination[key_cos] = value_cos , average_value\n",
    "\n",
    "\n",
    "low = {}\n",
    "high = {}\n",
    "for key,value in combination.items():\n",
    "    if key[2] == \"low\":\n",
    "        low[key] = value\n",
    "    else:\n",
    "        high[key] = value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity vs. Participant average:\n",
      "rho     = 0.8000\n",
      "p-value = 0.2000\n",
      "cosine similarity vs. Participant average:\n",
      "rho     = -0.8000\n",
      "p-value = 0.2000\n"
     ]
    }
   ],
   "source": [
    "values0 = []\n",
    "values1 =[]\n",
    "for key,value in high.items():\n",
    "    values0.append(value[0].tolist())\n",
    "    values1.append(value[1])\n",
    "flat_values0 = []\n",
    "for sublist in values0:\n",
    "    for num in sublist:\n",
    "        flat_values0.append(num)\n",
    "\n",
    "res = stats.spearmanr(flat_values0, values1)\n",
    "print(\"\"\"cosine similarity vs. Participant average:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(res[0], res[1]))\n",
    "\n",
    "values0 = []\n",
    "values1 =[]\n",
    "for key,value in low.items():\n",
    "    values0.append(value[0].tolist())\n",
    "    values1.append(value[1])\n",
    "\n",
    "flat_values0 = []\n",
    "for sublist in values0:\n",
    "    for num in sublist:\n",
    "        flat_values0.append(num)\n",
    "\n",
    "res2 = stats.spearmanr(flat_values0, values1)\n",
    "print(\"\"\"cosine similarity vs. Participant average:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(res2[0], res2[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Any comments/thoughts should go here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature\n",
    "\n",
    "[1] C. Silberer and M. Lapata. Learning grounded meaning representations with autoencoders. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721732, Baltimore, Maryland, USA, June 2325 2014 2014. Association for Computational Linguistics.  \n",
    "\n",
    "[2] Mitchell, J., & Lapata, M. (2008). Vector-based Models of Semantic Composition. In Proceedings of ACL-08: HLT (pp. 236244). Association for Computational Linguistics.\n",
    "  \n",
    "[3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 31113119, 2013.\n",
    "\n",
    "[4] E. Vylomova, L. Rimell, T. Cohn, and T. Baldwin. Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning. arXiv, arXiv:1509.01692 [cs.CL], 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We met once on Tuesday the 16th for 4-5 hours. Everyone was present and contributed equally on solving the assignment.\n",
    "We worked further on the assignment on Thursday during and after the help session (for around 3 hours) and the contributions were equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "The assignment is marked on a 7-level scale where 4 is sufficient to complete the assignment; 5 is good solid work; 6 is excellent work, covers most of the assignment; and 7: creative work. \n",
    "\n",
    "This assignment has a total of 60 marks. These translate to grades as follows: 1 = 17% 2 = 34%, 3 = 50%, 4 = 67%, 5 = 75%, 6 = 84%, 7 = 92% where %s are interpreted as lower bounds to achieve that grade."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
